# 11 â€” Mantenimiento & AuditorÃ­a

> **Tiempo estimado**: 2 dÃ­as (16 horas)
> 
> **Prerrequisitos**: MÃ³dulos 01-10 completados

---

## ğŸ¯ Objetivos del MÃ³dulo

Al completar este mÃ³dulo serÃ¡s capaz de:

1. âœ… Crear **playbooks de mantenimiento**
2. âœ… Implementar **tests de regresiÃ³n**
3. âœ… Gestionar **actualizaciÃ³n de dependencias**
4. âœ… Establecer **calendario de revisiones**

---

## ğŸ“– Contenido TeÃ³rico

### 1. Playbook de Mantenimiento

```markdown
# Playbook de Mantenimiento â€” ML System

## Tareas Rutinarias

### Diarias
- [ ] Revisar logs de errores
- [ ] Verificar mÃ©tricas de latencia
- [ ] Confirmar que health checks pasan

### Semanales
- [ ] Revisar drift reports
- [ ] Analizar distribuciÃ³n de predicciones
- [ ] Backup de MLflow artifacts

### Mensuales
- [ ] Evaluar modelo contra datos recientes
- [ ] Revisar y actualizar dependencias
- [ ] Regenerar lockfiles

### Trimestrales
- [ ] Reentrenar modelo si degradaciÃ³n > 5%
- [ ] AuditorÃ­a de sesgos
- [ ] Actualizar Model Card

---

## Runbooks

### Runbook: DegradaciÃ³n de MÃ©tricas

**SÃ­ntoma**: AUC/F1 cae mÃ¡s de 5% respecto a baseline

**Pasos**:
1. Verificar datos de entrada (drift detection)
2. Comparar distribuciÃ³n de features vs training
3. Si hay drift significativo â†’ reentrenar
4. Si no hay drift â†’ investigar cambios en upstream

### Runbook: Alta Latencia

**SÃ­ntoma**: p95 latency > 500ms

**Pasos**:
1. Verificar CPU/Memory del pod
2. Revisar tamaÃ±o de batch de requests
3. Verificar si hay queries lentas al modelo
4. Escalar horizontalmente si necesario

### Runbook: Rollback de Modelo

**SÃ­ntoma**: Nuevo modelo con problemas en producciÃ³n

**Pasos**:
1. Identificar versiÃ³n anterior estable
2. `mlflow models update-stage --model-name X --version Y --stage Production`
3. Restart de pods de inferencia
4. Verificar mÃ©tricas post-rollback
5. Documentar incidente
```

---

### 2. Tests de RegresiÃ³n

```python
"""tests/regression/test_model_regression.py â€” Tests de regresiÃ³n."""
import pytest
import json
import numpy as np
import pandas as pd
from pathlib import Path


class TestModelRegression:
    """Tests de regresiÃ³n del modelo."""
    
    @pytest.fixture
    def baseline_metrics(self) -> dict:
        """MÃ©tricas baseline guardadas."""
        baseline_path = Path("tests/regression/baseline_metrics.json")
        with open(baseline_path) as f:
            return json.load(f)
    
    @pytest.fixture
    def current_metrics(self, trained_pipeline, test_data) -> dict:
        """MÃ©tricas actuales."""
        from sklearn.metrics import accuracy_score, roc_auc_score, f1_score
        
        X_test = test_data.drop("churn", axis=1)
        y_test = test_data["churn"]
        
        y_pred = trained_pipeline.predict(X_test)
        y_proba = trained_pipeline.predict_proba(X_test)[:, 1]
        
        return {
            "accuracy": accuracy_score(y_test, y_pred),
            "roc_auc": roc_auc_score(y_test, y_proba),
            "f1": f1_score(y_test, y_pred),
        }
    
    def test_accuracy_not_degraded(self, baseline_metrics, current_metrics):
        """Accuracy no debe degradarse mÃ¡s de 5%."""
        degradation = baseline_metrics["accuracy"] - current_metrics["accuracy"]
        assert degradation < 0.05, f"DegradaciÃ³n de accuracy: {degradation:.2%}"
    
    def test_auc_not_degraded(self, baseline_metrics, current_metrics):
        """AUC no debe degradarse mÃ¡s de 5%."""
        degradation = baseline_metrics["roc_auc"] - current_metrics["roc_auc"]
        assert degradation < 0.05, f"DegradaciÃ³n de AUC: {degradation:.2%}"
    
    def test_f1_not_degraded(self, baseline_metrics, current_metrics):
        """F1 no debe degradarse mÃ¡s de 5%."""
        degradation = baseline_metrics["f1"] - current_metrics["f1"]
        assert degradation < 0.05, f"DegradaciÃ³n de F1: {degradation:.2%}"
    
    def test_prediction_distribution(self, trained_pipeline, test_data):
        """DistribuciÃ³n de predicciones debe ser razonable."""
        X_test = test_data.drop("churn", axis=1)
        y_proba = trained_pipeline.predict_proba(X_test)[:, 1]
        
        # No debe predecir todo 0 o todo 1
        assert 0.05 < np.mean(y_proba > 0.5) < 0.95
```

---

### 3. GestiÃ³n de Dependencias

```bash
# Verificar dependencias desactualizadas
pip list --outdated

# Actualizar de forma segura
pip install --upgrade package==X.Y.Z

# Regenerar lockfile
pip freeze > requirements.lock

# Auditar vulnerabilidades
pip-audit
safety check
```

#### Script de ActualizaciÃ³n

```python
"""scripts/update_deps.py â€” ActualizaciÃ³n de dependencias."""
import subprocess
import json
from datetime import datetime


def check_outdated() -> list[dict]:
    """Lista dependencias desactualizadas."""
    result = subprocess.run(
        ["pip", "list", "--outdated", "--format=json"],
        capture_output=True,
        text=True,
    )
    return json.loads(result.stdout)


def update_package(name: str, version: str) -> bool:
    """Actualiza un paquete especÃ­fico."""
    result = subprocess.run(
        ["pip", "install", f"{name}=={version}"],
        capture_output=True,
    )
    return result.returncode == 0


def run_tests() -> bool:
    """Ejecuta tests."""
    result = subprocess.run(
        ["pytest", "tests/", "-q"],
        capture_output=True,
    )
    return result.returncode == 0


def main():
    outdated = check_outdated()
    print(f"Encontradas {len(outdated)} dependencias desactualizadas")
    
    for pkg in outdated:
        name = pkg["name"]
        current = pkg["version"]
        latest = pkg["latest_version"]
        
        print(f"\n{name}: {current} â†’ {latest}")
        
        # Actualizar
        if update_package(name, latest):
            # Verificar que tests pasan
            if run_tests():
                print(f"  âœ… Actualizado exitosamente")
            else:
                print(f"  âŒ Tests fallaron, revertiendo...")
                update_package(name, current)
        else:
            print(f"  âŒ Error al actualizar")


if __name__ == "__main__":
    main()
```

---

### 4. Calendario de Revisiones

```markdown
# Calendario de Mantenimiento ML

## Revisiones AutomÃ¡ticas (CI/CD)
- Tests unitarios: En cada PR
- Coverage check: En cada PR
- Security scan: Diario
- Dependency audit: Semanal

## Revisiones Manuales

### Mensual (1er lunes del mes)
- [ ] Revisar mÃ©tricas de modelo vs baseline
- [ ] Analizar drift reports
- [ ] Actualizar dependencias menores
- [ ] Revisar y cerrar issues antiguos

### Trimestral (1er semana del trimestre)
- [ ] Reentrenar modelo con datos recientes
- [ ] AuditorÃ­a de sesgos
- [ ] Actualizar Model Card
- [ ] Revisar y actualizar documentaciÃ³n
- [ ] Actualizar dependencias mayores

### Semestral
- [ ] RevisiÃ³n de arquitectura
- [ ] EvaluaciÃ³n de nuevos algoritmos
- [ ] Benchmark contra soluciones alternativas
- [ ] ActualizaciÃ³n de infraestructura
```

---

## ğŸ”§ Mini-Proyecto: Sistema de Mantenimiento

### Objetivo

1. Crear MAINTENANCE_GUIDE.md
2. Implementar tests de regresiÃ³n
3. Crear script de validaciÃ³n
4. Definir calendario de revisiones

### Estructura

```
work/11_mantenimiento_auditoria/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ MAINTENANCE_GUIDE.md
â”‚   â””â”€â”€ RUNBOOKS.md
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ regression/
â”‚       â”œâ”€â”€ test_model_regression.py
â”‚       â””â”€â”€ baseline_metrics.json
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ validate_guide.sh
â”‚   â””â”€â”€ update_deps.py
â””â”€â”€ CALENDAR.md
```

### Criterios de Ã‰xito

- [ ] MAINTENANCE_GUIDE.md completo
- [ ] Tests de regresiÃ³n implementados
- [ ] Script de validaciÃ³n funcional
- [ ] Calendario definido

---

## âœ… ValidaciÃ³n

```bash
make check-11
```

---

## ğŸ‰ Â¡Felicitaciones!

Has completado todos los mÃ³dulos de la GuÃ­a MLOps v2. Ahora tienes las habilidades para:

- âœ… Construir pipelines ML profesionales
- âœ… Implementar CI/CD con testing
- âœ… Desplegar APIs y dashboards
- âœ… Monitorear y mantener sistemas ML

**Siguiente paso**: Aplica todo lo aprendido reproduciendo los proyectos del portafolio.

---

*Ãšltima actualizaciÃ³n: 2024-12*
