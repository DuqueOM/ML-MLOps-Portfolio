# 11. Testing para Machine Learning

## üéØ Objetivo del M√≥dulo

Dominar el testing en proyectos ML para alcanzar **80%+ de coverage** sin tests fr√°giles ni falsos positivos.

```
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                                              ‚ïë
‚ïë  üö® LA REALIDAD DEL ML SIN TESTS:                                            ‚ïë
‚ïë                                                                              ‚ïë
‚ïë  "El modelo funcionaba ayer, hoy da predicciones random"                     ‚ïë
‚ïë  "Cambi√© una l√≠nea y romp√≠ todo el pipeline"                                 ‚ïë
‚ïë  "No s√© si el bug est√° en los datos, el preprocesamiento, o el modelo"       ‚ïë
‚ïë                                                                              ‚ïë
‚ïë  üõ°Ô∏è LA REALIDAD CON TESTS:                                                   ‚ïë
‚ïë                                                                              ‚ïë
‚ïë  "CI me avis√≥ que romp√≠ algo antes de hacer merge"                           ‚ïë
‚ïë  "S√© exactamente qu√© componente fall√≥"                                       ‚ïë
‚ïë  "Puedo refactorizar con confianza"                                          ‚ïë
‚ïë                                                                              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
```

---

## üìã Contenido

1. [La Pir√°mide de Testing en ML](#111-la-pir√°mide-de-testing-en-ml)
2. [Fixtures y conftest.py](#112-fixtures-y-conftestpy)
3. [Unit Tests: Funciones Individuales](#113-unit-tests-funciones-individuales)
4. [Data Tests: Validaci√≥n de Datos](#114-data-tests-validaci√≥n-de-datos)
5. [Model Tests: Comportamiento del Modelo](#115-model-tests-comportamiento-del-modelo)
6. [Integration Tests: Pipeline Completo](#116-integration-tests-pipeline-completo)
7. [Alcanzar 80% Coverage](#117-alcanzar-80-coverage)

### üß© C√≥mo se aplica en este portafolio

- Cada uno de los tres proyectos tiene una carpeta `tests/` rica en ejemplos reales:
  - **BankChurn-Predictor**: tests de pipeline de entrenamiento y m√©tricas.
  - **CarVision-Market-Intelligence**: tests de features, datos y modelo (incluidos en este m√≥dulo).
  - **TelecomAI-Customer-Intelligence**: tests centrados en clasificaci√≥n y contratos de datos.
- El workflow `ci-mlops.yml` ejecuta estos tests en matrix (3 proyectos √ó 2 versiones de Python)
  y aplica thresholds de coverage (79‚Äì80%). Este m√≥dulo te da el modelo mental para entender
  y extender esos tests sin romper la pir√°mide de testing.

---

## 11.1 La Pir√°mide de Testing en ML

### Analog√≠a: Inspecci√≥n de un Avi√≥n

```
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úàÔ∏è ANTES DE CADA VUELO, SE INSPECCIONA:                                  ‚ïë
‚ïë                                                                           ‚ïë
‚ïë  NIVEL 1 - Componentes individuales (Unit Tests):                         ‚ïë
‚ïë  ‚Ä¢ Cada tornillo est√° apretado                                            ‚ïë
‚ïë  ‚Ä¢ Cada cable est√° conectado                                              ‚ïë
‚ïë  ‚Ä¢ Cada sensor funciona                                                   ‚ïë
‚ïë                                                                           ‚ïë
‚ïë  NIVEL 2 - Sistemas (Integration Tests):                                  ‚ïë
‚ïë  ‚Ä¢ El motor arranca correctamente                                         ‚ïë
‚ïë  ‚Ä¢ Los flaps responden a los controles                                    ‚ïë
‚ïë  ‚Ä¢ El sistema hidr√°ulico mantiene presi√≥n                                 ‚ïë
‚ïë                                                                           ‚ïë
‚ïë  NIVEL 3 - Vuelo de prueba (E2E Tests):                                   ‚ïë
‚ïë  ‚Ä¢ El avi√≥n despega, vuela, y aterriza                                    ‚ïë
‚ïë  ‚Ä¢ Todo funciona junto bajo condiciones reales                            ‚ïë
‚ïë                                                                           ‚ïë
‚ïë  EN ML ES IGUAL: Testeas componentes ‚Üí sistemas ‚Üí pipeline completo       ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
```

### La Pir√°mide Espec√≠fica para ML

```
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚îÇ    E2E Tests    ‚îÇ ‚Üê 5-10% de tests
                        ‚îÇ   (API real)    ‚îÇ   Lentos, costosos
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   test_api_e2e.py
                                 ‚îÇ
                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                     ‚îÇ   Integration Tests   ‚îÇ ‚Üê 15-20% de tests
                     ‚îÇ  (pipeline.fit())     ‚îÇ   Verifican interacci√≥n
                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   test_training.py
                                 ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ           Model Tests               ‚îÇ ‚Üê 20-25% de tests
              ‚îÇ  (predicciones, m√©tricas, shapes)   ‚îÇ   Espec√≠ficos de ML
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   test_model_logic.py
                                 ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                 Data Tests                      ‚îÇ ‚Üê 20-25% de tests
        ‚îÇ    (schema, rangos, distribuciones, NaN)        ‚îÇ   Validan datos
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   test_data.py
                                 ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                           Unit Tests                                    ‚îÇ ‚Üê 30-40% de tests
‚îÇ              (funciones individuales, transformers)                     ‚îÇ   R√°pidos, muchos
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   test_features.py
```

### Coverage del Portafolio Real

| Proyecto | Coverage | Tests | Tipo Principal |
|----------|:--------:|:-----:|----------------|
| **BankChurn** | 79.5% | 45+ | Unit + Integration |
| **CarVision** | 97% | 50+ | Unit + Data + Model |
| **TelecomAI** | 97% | 35+ | Unit + Integration |

---

## 11.2 Fixtures y conftest.py

### ¬øQu√© es una Fixture?

```
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  üß™ FIXTURE = Datos o recursos preparados para tests                      ‚ïë
‚ïë                                                                           ‚ïë
‚ïë  Analog√≠a del laboratorio:                                                ‚ïë
‚ïë  ‚Ä¢ Antes de cada experimento, preparas tus instrumentos                   ‚ïë
‚ïë  ‚Ä¢ Los instrumentos son los mismos para varios experimentos               ‚ïë
‚ïë  ‚Ä¢ No los preparas desde cero cada vez                                    ‚ïë
‚ïë                                                                           ‚ïë
‚ïë  En pytest:                                                               ‚ïë
‚ïë  ‚Ä¢ Fixture prepara datos/modelos/configs                                  ‚ïë
‚ïë  ‚Ä¢ Se reutiliza en m√∫ltiples tests                                        ‚ïë
‚ïë  ‚Ä¢ Se limpia autom√°ticamente despu√©s                                      ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
```

### conftest.py del Portafolio (CarVision)

```python
# tests/conftest.py - C√≥digo REAL del portafolio

import pytest
import pandas as pd
import numpy as np
from pathlib import Path
import tempfile
import yaml

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# FIXTURES DE DATOS
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@pytest.fixture
def sample_data() -> pd.DataFrame:
    """DataFrame peque√±o para tests r√°pidos.
    
    Este fixture se usa en MUCHOS tests:
    - test_data.py: Verificar carga y limpieza
    - test_features.py: Verificar feature engineering
    - test_model.py: Verificar predicciones
    """
    return pd.DataFrame({
        "price": [15000, 25000, 35000, 45000, 55000],
        "model_year": [2015, 2018, 2020, 2019, 2021],
        "odometer": [80000, 45000, 20000, 30000, 10000],
        "model": ["ford f-150", "toyota camry", "honda civic", 
                  "chevrolet silverado", "ford mustang"],
        "fuel": ["gas", "gas", "gas", "diesel", "gas"],
        "transmission": ["automatic", "automatic", "manual", 
                        "automatic", "manual"],
        "condition": ["good", "excellent", "like new", 
                     "good", "excellent"],
    })


@pytest.fixture
def sample_data_with_nulls() -> pd.DataFrame:
    """DataFrame con valores faltantes para probar imputaci√≥n."""
    return pd.DataFrame({
        "price": [15000, None, 35000, 45000, None],
        "model_year": [2015, 2018, None, 2019, 2021],
        "odometer": [80000, None, 20000, 30000, 10000],
        "model": ["ford f-150", None, "honda civic", None, "ford mustang"],
        "fuel": ["gas", "gas", None, "diesel", "gas"],
        "transmission": [None, "automatic", "manual", "automatic", None],
    })


@pytest.fixture
def large_sample_data() -> pd.DataFrame:
    """DataFrame m√°s grande para tests de performance."""
    np.random.seed(42)
    n = 1000
    return pd.DataFrame({
        "price": np.random.uniform(5000, 80000, n),
        "model_year": np.random.randint(2010, 2024, n),
        "odometer": np.random.uniform(0, 200000, n),
        "model": np.random.choice(["ford f-150", "toyota camry", "honda civic"], n),
        "fuel": np.random.choice(["gas", "diesel", "electric"], n),
        "transmission": np.random.choice(["automatic", "manual"], n),
    })


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# FIXTURES DE CONFIGURACI√ìN
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@pytest.fixture
def sample_config() -> dict:
    """Configuraci√≥n m√≠nima para tests."""
    return {
        "seed": 42,
        "dataset_year": 2024,
        "paths": {
            "data_path": "data/raw/vehicles_us.csv",
            "artifacts_dir": "artifacts",
            "model_path": "artifacts/model.joblib",
        },
        "preprocessing": {
            "numeric_features": ["odometer", "vehicle_age"],
            "categorical_features": ["fuel", "transmission", "brand"],
            "drop_columns": ["price_per_mile", "price_category"],
            "filters": {
                "price_min": 1000,
                "price_max": 100000,
            }
        },
        "training": {
            "target": "price",
            "test_size": 0.2,
            "val_size": 0.1,
            "shuffle": True,
            "model": "random_forest",
            "random_forest_params": {
                "n_estimators": 10,  # Pocos para tests r√°pidos
                "max_depth": 5,
                "random_state": 42,
            }
        }
    }


@pytest.fixture
def temp_config_file(sample_config, tmp_path) -> Path:
    """Crea archivo config temporal para tests de carga."""
    config_path = tmp_path / "config.yaml"
    with open(config_path, "w") as f:
        yaml.dump(sample_config, f)
    return config_path


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# FIXTURES DE MODELO
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@pytest.fixture
def fitted_pipeline(sample_data, sample_config):
    """Pipeline entrenado para tests de predicci√≥n."""
    from src.carvision.training import build_pipeline
    from src.carvision.features import FeatureEngineer
    
    # Preparar datos
    fe = FeatureEngineer(current_year=2024)
    df = fe.transform(sample_data)
    
    X = df.drop(columns=["price"])
    y = df["price"]
    
    # Construir y entrenar pipeline
    # Nota: Usamos config simplificada para velocidad
    pipeline = build_pipeline(sample_config)
    pipeline.fit(X, y)
    
    return pipeline


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# FIXTURES ESPECIALES
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@pytest.fixture
def temp_artifacts_dir(tmp_path) -> Path:
    """Directorio temporal para artefactos."""
    artifacts = tmp_path / "artifacts"
    artifacts.mkdir()
    return artifacts


@pytest.fixture(scope="module")
def slow_fixture():
    """Fixture que tarda en crearse - se reutiliza en todo el m√≥dulo.
    
    scope="module" significa que se crea UNA vez por archivo de test,
    no una vez por cada test.
    """
    import time
    time.sleep(0.1)  # Simula operaci√≥n lenta
    return {"expensive_resource": True}
```

### Uso de Fixtures en Tests

```python
# tests/test_features.py

def test_feature_engineer_creates_vehicle_age(sample_data):
    """Test que usa la fixture sample_data."""
    from src.carvision.features import FeatureEngineer
    
    fe = FeatureEngineer(current_year=2024)
    result = fe.transform(sample_data)
    
    assert "vehicle_age" in result.columns
    assert result["vehicle_age"].iloc[0] == 2024 - 2015  # 9 a√±os


def test_pipeline_predicts_positive_prices(fitted_pipeline, sample_data):
    """Test que usa DOS fixtures."""
    X = sample_data.drop(columns=["price"])
    predictions = fitted_pipeline.predict(X)
    
    assert all(predictions > 0), "Precios deben ser positivos"
```

---

## 11.3 Unit Tests: Funciones Individuales

### Qu√© Testear en Unit Tests

```python
# tests/test_features.py - C√≥digo REAL del portafolio

import pytest
import pandas as pd
from src.carvision.features import FeatureEngineer


class TestFeatureEngineer:
    """Tests unitarios para FeatureEngineer."""
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # TEST: Creaci√≥n de features
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def test_creates_vehicle_age(self, sample_data):
        """Verifica que vehicle_age se calcula correctamente."""
        fe = FeatureEngineer(current_year=2024)
        result = fe.transform(sample_data)
        
        assert "vehicle_age" in result.columns
        # 2024 - 2015 = 9 a√±os para el primer registro
        assert result.loc[0, "vehicle_age"] == 9
    
    def test_creates_brand_from_model(self, sample_data):
        """Verifica que brand extrae la primera palabra de model."""
        fe = FeatureEngineer(current_year=2024)
        result = fe.transform(sample_data)
        
        assert "brand" in result.columns
        assert result.loc[0, "brand"] == "ford"  # "ford f-150" ‚Üí "ford"
        assert result.loc[1, "brand"] == "toyota"  # "toyota camry" ‚Üí "toyota"
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # TEST: Manejo de edge cases
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def test_handles_missing_model_year(self):
        """Verifica comportamiento con model_year faltante."""
        df = pd.DataFrame({
            "price": [15000],
            "model": ["ford f-150"],
            # Sin model_year
        })
        
        fe = FeatureEngineer(current_year=2024)
        result = fe.transform(df)
        
        # No debe crear vehicle_age si no hay model_year
        assert "vehicle_age" not in result.columns
    
    def test_handles_missing_model_column(self):
        """Verifica comportamiento sin columna model."""
        df = pd.DataFrame({
            "price": [15000],
            "model_year": [2015],
            # Sin model
        })
        
        fe = FeatureEngineer(current_year=2024)
        result = fe.transform(df)
        
        # No debe crear brand si no hay model
        assert "brand" not in result.columns
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # TEST: Inmutabilidad
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def test_does_not_modify_input(self, sample_data):
        """Verifica que el DataFrame original no se modifica."""
        original_columns = sample_data.columns.tolist()
        original_values = sample_data.copy()
        
        fe = FeatureEngineer(current_year=2024)
        _ = fe.transform(sample_data)
        
        # Columnas originales sin cambio
        assert sample_data.columns.tolist() == original_columns
        # Valores originales sin cambio
        pd.testing.assert_frame_equal(sample_data, original_values)
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # TEST: Compatibilidad con sklearn
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def test_fit_returns_self(self, sample_data):
        """Verifica que fit() retorna self (requerido por sklearn)."""
        fe = FeatureEngineer(current_year=2024)
        result = fe.fit(sample_data)
        
        assert result is fe
    
    def test_works_in_pipeline(self, sample_data):
        """Verifica que funciona dentro de un Pipeline sklearn."""
        from sklearn.pipeline import Pipeline
        from sklearn.preprocessing import StandardScaler
        
        pipe = Pipeline([
            ("features", FeatureEngineer(current_year=2024)),
            ("scaler", StandardScaler())
        ])
        
        # No debe lanzar excepci√≥n
        # (Solo probamos que no falla, no el resultado)
        try:
            # Seleccionar solo columnas num√©ricas para StandardScaler
            numeric_cols = ["price", "model_year", "odometer"]
            result = pipe.fit_transform(sample_data[numeric_cols])
            assert result is not None
        except Exception as e:
            pytest.fail(f"Pipeline fall√≥: {e}")
```

---

## 11.4 Data Tests: Validaci√≥n de Datos

### Tests de Schema y Calidad de Datos

```python
# tests/test_data.py - C√≥digo REAL del portafolio

import pytest
import pandas as pd
import numpy as np
from src.carvision.data import load_data, clean_data


class TestDataLoading:
    """Tests de carga y validaci√≥n de datos."""
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # TEST: Schema validation
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def test_required_columns_exist(self, sample_data):
        """Verifica que existen las columnas requeridas."""
        required = ["price", "model_year", "odometer", "model", "fuel"]
        
        for col in required:
            assert col in sample_data.columns, f"Falta columna: {col}"
    
    def test_column_types(self, sample_data):
        """Verifica tipos de datos correctos."""
        # Num√©ricas
        assert pd.api.types.is_numeric_dtype(sample_data["price"])
        assert pd.api.types.is_numeric_dtype(sample_data["model_year"])
        assert pd.api.types.is_numeric_dtype(sample_data["odometer"])
        
        # Categ√≥ricas/String
        assert pd.api.types.is_object_dtype(sample_data["model"])
        assert pd.api.types.is_object_dtype(sample_data["fuel"])
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # TEST: Value ranges
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def test_price_is_positive(self, sample_data):
        """Verifica que precios son positivos."""
        assert (sample_data["price"] > 0).all(), "Hay precios <= 0"
    
    def test_price_in_reasonable_range(self, sample_data):
        """Verifica que precios est√°n en rango razonable."""
        assert sample_data["price"].min() >= 100, "Precio muy bajo (posible error)"
        assert sample_data["price"].max() <= 500000, "Precio muy alto (posible error)"
    
    def test_model_year_in_range(self, sample_data):
        """Verifica que a√±os son razonables."""
        current_year = pd.Timestamp.now().year
        
        assert sample_data["model_year"].min() >= 1900, "A√±o muy antiguo"
        assert sample_data["model_year"].max() <= current_year + 1, "A√±o futuro"
    
    def test_odometer_is_non_negative(self, sample_data):
        """Verifica que odometer no es negativo."""
        assert (sample_data["odometer"] >= 0).all(), "Hay odometer negativo"
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # TEST: Categorical values
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def test_fuel_valid_values(self, sample_data):
        """Verifica que fuel tiene valores v√°lidos."""
        valid_fuels = {"gas", "diesel", "electric", "hybrid", "other"}
        actual_fuels = set(sample_data["fuel"].dropna().unique())
        
        invalid = actual_fuels - valid_fuels
        assert len(invalid) == 0, f"Valores de fuel inv√°lidos: {invalid}"
    
    def test_transmission_valid_values(self, sample_data):
        """Verifica que transmission tiene valores v√°lidos."""
        valid = {"automatic", "manual", "other"}
        actual = set(sample_data["transmission"].dropna().unique())
        
        invalid = actual - valid
        assert len(invalid) == 0, f"Valores de transmission inv√°lidos: {invalid}"


class TestDataCleaning:
    """Tests de limpieza de datos."""
    
    def test_clean_data_removes_invalid_prices(self, sample_data):
        """Verifica que clean_data filtra precios fuera de rango."""
        # A√±adir registro con precio inv√°lido
        bad_data = pd.concat([
            sample_data,
            pd.DataFrame({"price": [100], "model_year": [2020], 
                         "odometer": [1000], "model": ["test"],
                         "fuel": ["gas"], "transmission": ["automatic"],
                         "condition": ["good"]})
        ], ignore_index=True)
        
        filters = {"price_min": 1000, "price_max": 100000}
        cleaned = clean_data(bad_data, filters=filters)
        
        # El registro con price=100 debe ser eliminado
        assert len(cleaned) == len(sample_data)
        assert (cleaned["price"] >= 1000).all()
    
    def test_clean_data_handles_nulls(self, sample_data_with_nulls):
        """Verifica que clean_data no falla con NaN."""
        # No debe lanzar excepci√≥n
        filters = {"price_min": 1000}
        cleaned = clean_data(sample_data_with_nulls, filters=filters)
        
        assert cleaned is not None
        # Registros con price=None deben ser eliminados o manejados
        assert len(cleaned) <= len(sample_data_with_nulls)
```

---

## 11.5 Model Tests: Comportamiento del Modelo

### Tests Espec√≠ficos de ML

```python
# tests/test_model_logic.py - C√≥digo REAL del portafolio

import pytest
import numpy as np
import pandas as pd


class TestModelPredictions:
    """Tests de comportamiento del modelo."""
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # TEST: Output shape y tipo
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def test_predict_returns_correct_shape(self, fitted_pipeline, sample_data):
        """Verifica que predict retorna un array del tama√±o correcto."""
        X = sample_data.drop(columns=["price"])
        predictions = fitted_pipeline.predict(X)
        
        assert len(predictions) == len(X), "N√∫mero de predicciones incorrecto"
    
    def test_predict_returns_numeric(self, fitted_pipeline, sample_data):
        """Verifica que predicciones son num√©ricas."""
        X = sample_data.drop(columns=["price"])
        predictions = fitted_pipeline.predict(X)
        
        assert np.issubdtype(predictions.dtype, np.number), "Predicciones no son num√©ricas"
    
    def test_predict_no_nan(self, fitted_pipeline, sample_data):
        """Verifica que no hay NaN en predicciones."""
        X = sample_data.drop(columns=["price"])
        predictions = fitted_pipeline.predict(X)
        
        assert not np.isnan(predictions).any(), "Hay NaN en predicciones"
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # TEST: Rangos razonables
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def test_predictions_are_positive(self, fitted_pipeline, sample_data):
        """Verifica que precios predichos son positivos."""
        X = sample_data.drop(columns=["price"])
        predictions = fitted_pipeline.predict(X)
        
        assert (predictions > 0).all(), "Hay predicciones <= 0"
    
    def test_predictions_in_training_range(self, fitted_pipeline, sample_data):
        """Verifica que predicciones est√°n en rango similar al training."""
        X = sample_data.drop(columns=["price"])
        y = sample_data["price"]
        predictions = fitted_pipeline.predict(X)
        
        # Predicciones deben estar dentro de un margen razonable
        min_price = y.min() * 0.1  # 10% del m√≠nimo
        max_price = y.max() * 3.0  # 300% del m√°ximo
        
        assert predictions.min() >= min_price, "Predicci√≥n muy baja"
        assert predictions.max() <= max_price, "Predicci√≥n muy alta"
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # TEST: Consistencia (determinismo)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def test_predictions_are_deterministic(self, fitted_pipeline, sample_data):
        """Verifica que mismos inputs dan mismos outputs."""
        X = sample_data.drop(columns=["price"])
        
        pred1 = fitted_pipeline.predict(X)
        pred2 = fitted_pipeline.predict(X)
        
        np.testing.assert_array_equal(pred1, pred2, 
            "Predicciones no son determin√≠sticas")
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # TEST: Sensibilidad a features
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def test_higher_odometer_lower_price(self, fitted_pipeline, sample_data):
        """Verifica que mayor odometer tiende a menor precio.
        
        Este es un test de "sanity check": verifica que el modelo
        aprendi√≥ relaciones b√°sicas del dominio.
        """
        X = sample_data.drop(columns=["price"]).copy()
        
        # Predicci√≥n con odometer original
        pred_low_odo = fitted_pipeline.predict(X)
        
        # Aumentar odometer significativamente
        X_high_odo = X.copy()
        X_high_odo["odometer"] = X["odometer"] * 3
        pred_high_odo = fitted_pipeline.predict(X_high_odo)
        
        # En promedio, mayor odometer ‚Üí menor precio
        # (No requiere que TODOS sean menores, solo el promedio)
        assert pred_high_odo.mean() < pred_low_odo.mean(), \
            "Modelo no aprendi√≥ que mayor odometer = menor precio"
    
    def test_newer_car_higher_price(self, fitted_pipeline, sample_data):
        """Verifica que autos m√°s nuevos tienden a mayor precio."""
        X = sample_data.drop(columns=["price"]).copy()
        
        # Predicci√≥n con model_year original
        pred_original = fitted_pipeline.predict(X)
        
        # Hacer autos 5 a√±os m√°s nuevos
        X_newer = X.copy()
        X_newer["model_year"] = X["model_year"] + 5
        pred_newer = fitted_pipeline.predict(X_newer)
        
        # En promedio, m√°s nuevo ‚Üí mayor precio
        assert pred_newer.mean() > pred_original.mean(), \
            "Modelo no aprendi√≥ que autos nuevos cuestan m√°s"


class TestModelMetrics:
    """Tests de m√©tricas del modelo."""
    
    def test_rmse_below_threshold(self, fitted_pipeline, sample_data):
        """Verifica que RMSE est√° por debajo de umbral aceptable."""
        from sklearn.metrics import mean_squared_error
        
        X = sample_data.drop(columns=["price"])
        y = sample_data["price"]
        predictions = fitted_pipeline.predict(X)
        
        rmse = np.sqrt(mean_squared_error(y, predictions))
        
        # RMSE debe ser menor que el 50% del precio promedio
        # (umbral arbitrario para sample data peque√±o)
        threshold = y.mean() * 0.5
        assert rmse < threshold, f"RMSE={rmse:.2f} > threshold={threshold:.2f}"
    
    def test_r2_above_threshold(self, fitted_pipeline, sample_data):
        """Verifica que R¬≤ est√° por encima de umbral m√≠nimo."""
        from sklearn.metrics import r2_score
        
        X = sample_data.drop(columns=["price"])
        y = sample_data["price"]
        predictions = fitted_pipeline.predict(X)
        
        r2 = r2_score(y, predictions)
        
        # R¬≤ debe ser positivo (mejor que predecir la media)
        # Nota: Con sample data peque√±o, R¬≤ puede ser bajo
        assert r2 > 0.0, f"R¬≤={r2:.3f} <= 0 (peor que baseline)"
```

---

## 11.6 Integration Tests: Pipeline Completo

### Tests End-to-End

```python
# tests/test_main_workflow.py - C√≥digo REAL del portafolio

import pytest
import pandas as pd
import numpy as np
from pathlib import Path
import tempfile
import joblib


class TestTrainingWorkflow:
    """Tests de integraci√≥n del flujo completo de entrenamiento."""
    
    def test_full_training_pipeline(self, sample_data, sample_config, tmp_path):
        """Test end-to-end: datos ‚Üí entrenamiento ‚Üí modelo guardado."""
        from src.carvision.training import train_model
        
        # Configurar paths temporales
        sample_config["paths"]["artifacts_dir"] = str(tmp_path)
        sample_config["paths"]["model_path"] = str(tmp_path / "model.joblib")
        
        # Guardar datos temporales
        data_path = tmp_path / "data.csv"
        sample_data.to_csv(data_path, index=False)
        sample_config["paths"]["data_path"] = str(data_path)
        
        # Ejecutar entrenamiento
        result = train_model(sample_config)
        
        # Verificaciones
        assert "rmse" in result, "Falta m√©trica RMSE"
        assert result["rmse"] > 0, "RMSE debe ser positivo"
        
        # Verificar que modelo se guard√≥
        model_path = Path(sample_config["paths"]["model_path"])
        assert model_path.exists(), "Modelo no se guard√≥"
        
        # Verificar que modelo se puede cargar
        model = joblib.load(model_path)
        assert model is not None, "Modelo no se puede cargar"
    
    def test_training_creates_all_artifacts(self, sample_data, sample_config, tmp_path):
        """Verifica que entrenamiento crea todos los artefactos esperados."""
        from src.carvision.training import train_model
        
        # Setup
        sample_config["paths"]["artifacts_dir"] = str(tmp_path)
        sample_config["paths"]["model_path"] = str(tmp_path / "model.joblib")
        sample_config["paths"]["metrics_path"] = str(tmp_path / "metrics.json")
        
        data_path = tmp_path / "data.csv"
        sample_data.to_csv(data_path, index=False)
        sample_config["paths"]["data_path"] = str(data_path)
        
        # Train
        train_model(sample_config)
        
        # Verificar artefactos
        assert (tmp_path / "model.joblib").exists(), "Falta model.joblib"
        # metrics.json es opcional en algunos configs
    
    def test_loaded_model_predicts_correctly(self, sample_data, sample_config, tmp_path):
        """Verifica que modelo guardado predice igual que antes de guardar."""
        from src.carvision.training import train_model, build_pipeline
        from src.carvision.features import FeatureEngineer
        
        # Setup y entrenamiento
        sample_config["paths"]["artifacts_dir"] = str(tmp_path)
        model_path = tmp_path / "model.joblib"
        sample_config["paths"]["model_path"] = str(model_path)
        
        data_path = tmp_path / "data.csv"
        sample_data.to_csv(data_path, index=False)
        sample_config["paths"]["data_path"] = str(data_path)
        
        train_model(sample_config)
        
        # Cargar modelo
        loaded_model = joblib.load(model_path)
        
        # Predecir con nuevo dato
        new_data = pd.DataFrame({
            "model_year": [2020],
            "odometer": [30000],
            "model": ["ford f-150"],
            "fuel": ["gas"],
            "transmission": ["automatic"],
        })
        
        prediction = loaded_model.predict(new_data)
        
        # Verificaciones b√°sicas
        assert len(prediction) == 1
        assert prediction[0] > 0
        assert not np.isnan(prediction[0])


class TestAPIWorkflow:
    """Tests de integraci√≥n del API."""
    
    @pytest.mark.slow
    def test_api_prediction_endpoint(self, fitted_pipeline, tmp_path):
        """Test E2E del endpoint de predicci√≥n."""
        from fastapi.testclient import TestClient
        import sys
        
        # Guardar modelo para el API
        model_path = tmp_path / "model.joblib"
        joblib.dump(fitted_pipeline, model_path)
        
        # Importar app (puede requerir configuraci√≥n de paths)
        # Este test asume que ARTIFACTS_DIR est√° configurado
        import os
        os.environ["ARTIFACTS_DIR"] = str(tmp_path)
        
        try:
            from app.fastapi_app import app
            client = TestClient(app)
            
            # Request de predicci√≥n
            response = client.post("/predict", json={
                "model_year": 2020,
                "odometer": 30000,
                "model": "ford f-150",
                "fuel": "gas",
                "transmission": "automatic"
            })
            
            assert response.status_code == 200
            data = response.json()
            assert "prediction" in data
            assert data["prediction"] > 0
        except ImportError:
            pytest.skip("FastAPI app not available")
```

---

## üß® Errores habituales y c√≥mo depurarlos en testing ML

Aunque pytest es muy potente, en ML es f√°cil caer en tests fr√°giles o enga√±osos. Estos son los patrones m√°s comunes y c√≥mo atacarlos.

### 1) Tests que dependen de datos reales o externos

**S√≠ntomas t√≠picos**

- Tests que leen de `data/raw/...` o llaman APIs externas.
- Fallan solo en CI o solo en ciertas m√°quinas.

**C√≥mo identificarlo**

- Busca en tests accesos directos a rutas del proyecto o a recursos externos.
- Revisa que tus fixtures (`sample_data`, `sample_config`, etc.) no lean de archivos reales salvo cuando se prueban funciones de I/O.

**C√≥mo corregirlo**

- Usa **fixtures sint√©ticas** en memoria para la mayor√≠a de tests.
- Deja el acceso a disco/red solo en tests de integraci√≥n marcados (`@pytest.mark.integration` o `@pytest.mark.slow`).

---

### 2) Fixtures mal definidas (estado compartido, side-effects)

**S√≠ntomas t√≠picos**

- Tests que pasan individualmente pero fallan cuando se ejecutan todos juntos.
- Mutaci√≥n de `DataFrame` compartido entre tests.

**C√≥mo identificarlo**

- Revisa que tus fixtures devuelvan **nuevas instancias** o copias (`df.copy()`) cuando sea necesario.
- Usa `scope="function"` por defecto; solo usa `module`/`session` para recursos pesados cuidadosamente dise√±ados.

**C√≥mo corregirlo**

- Asegura inmutabilidad en el c√≥digo (ej. `X = X.copy()` en transformers) y en los tests (no reutilizar el mismo objeto mutable entre m√∫ltiples asserts sin reset).

---

### 3) Coverage alto pero sin cubrir lo importante

**S√≠ntomas t√≠picos**

- Reporte de coverage muestra 80‚Äì90%, pero:
  - No hay tests de datos.
  - No hay tests de comportamiento del modelo.
  - Solo se testea ‚Äúfeliz path‚Äù.

**C√≥mo identificarlo**

- Usa `--cov-report=term-missing` para ver **qu√© l√≠neas** no se cubren.
- Verifica que las capas clave (data, features, training, prediction) tienen tests dedicados.

**C√≥mo corregirlo**

- A√±ade tests para:
  - Validaci√≥n de datos (rangos, NaN, tipos).
  - Comportamiento b√°sico del modelo (shapes, ranges, determinismo).
  - Al menos un flujo de integraci√≥n (`train_model`, API `/predict`).

---

### 4) Tests lentos que bloquean el flujo de trabajo

**S√≠ntomas t√≠picos**

- `pytest` tarda minutos porque ejecuta entrenamiento completo en cada test.

**C√≥mo identificarlo**

- Localiza tests que entrenan modelos con muchos datos o hiperpar√°metros pesados.

**C√≥mo corregirlo**

- Para tests, usa:
  - **datasets peque√±os**.
  - Modelos simplificados (pocos √°rboles, menor profundidad).
  - Marcadores `@pytest.mark.slow` para separar tests pesados.
- Ajusta tu CI para ejecutar r√°pida y frecuentemente los tests r√°pidos, y los lentos solo en ciertas ramas.

---

### 5) Patr√≥n general de debugging en tests ML

1. **Reproduce el fallo en local** con el mismo comando que CI (`pytest` con las mismas flags).
2. **A√≠sla el test problem√°tico** usando `-k` o el nombre del test.
3. **Inspecciona fixtures y datos**: aseg√∫rate de que no hay estado compartido inesperado.
4. **Conecta el problema** con la capa correspondiente (datos, features, modelo, API) y ajusta los tests para cubrir el caso real que quieres proteger.

Con esta mentalidad, los tests dejan de ser una carga y se convierten en la red de seguridad que te permite refactorizar con confianza.

---

## 11.7 Alcanzar 80% Coverage

### Configuraci√≥n de pytest-cov

```toml
# pyproject.toml

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = [
    "-v",
    "--cov=src/carvision",
    "--cov-report=term-missing",
    "--cov-report=html",
    "--cov-fail-under=80",
]
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests",
]

[tool.coverage.run]
source = ["src"]
omit = [
    "tests/*",
    "*/__init__.py",
    "*/visualization.py",  # Excluir c√≥digo de UI
]

[tool.coverage.report]
fail_under = 80
exclude_lines = [
    "pragma: no cover",
    "if __name__ == .__main__.:",
    "raise NotImplementedError",
]
```

### Ejecutar Tests con Coverage

```bash
# Tests r√°pidos (sin slow)
pytest -m "not slow"

# Todos los tests con coverage
pytest --cov=src/carvision --cov-report=term-missing

# Solo ver coverage sin ejecutar tests
pytest --cov=src/carvision --cov-report=html
# Abre htmlcov/index.html en el navegador

# Verificar que pasa el threshold
pytest --cov-fail-under=80
```

### Estrategias para Aumentar Coverage

```
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  üìà C√ìMO PASAR DE 60% A 80% COVERAGE                                      ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë                                                                           ‚ïë
‚ïë  1. IDENTIFICAR GAPS                                                      ‚ïë
‚ïë     pytest --cov-report=term-missing                                      ‚ïë
‚ïë     ‚Üí Muestra l√≠neas NO cubiertas                                         ‚ïë
‚ïë                                                                           ‚ïë
‚ïë  2. PRIORIZAR                                                             ‚ïë
‚ïë     ‚Ä¢ L√≥gica de negocio cr√≠tica (training, prediction)                    ‚ïë
‚ïë     ‚Ä¢ C√≥digo que maneja errores                                           ‚ïë
‚ïë     ‚Ä¢ Branches condicionales (if/else)                                    ‚ïë
‚ïë                                                                           ‚ïë
‚ïë  3. EXCLUIR LO QUE NO VALE LA PENA                                        ‚ïë
‚ïë     ‚Ä¢ C√≥digo de visualizaci√≥n (Streamlit, plots)                          ‚ïë
‚ïë     ‚Ä¢ Scripts de utilidad one-off                                         ‚ïë
‚ïë     ‚Ä¢ C√≥digo de terceros                                                  ‚ïë
‚ïë                                                                           ‚ïë
‚ïë  4. TESTEAR EDGE CASES                                                    ‚ïë
‚ïë     ‚Ä¢ ¬øQu√© pasa con NaN?                                                  ‚ïë
‚ïë     ‚Ä¢ ¬øQu√© pasa con lista vac√≠a?                                          ‚ïë
‚ïë     ‚Ä¢ ¬øQu√© pasa con tipos incorrectos?                                    ‚ïë
‚ïë                                                                           ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
```

---

## ‚úÖ Checkpoint: ¬øCompletaste el M√≥dulo?

Antes de continuar, verifica:

- [ ] Tienes `tests/conftest.py` con fixtures reutilizables
- [ ] Tienes tests unitarios para tus transformers
- [ ] Tienes tests de validaci√≥n de datos
- [ ] Tienes tests de comportamiento del modelo
- [ ] Tienes al menos un test de integraci√≥n
- [ ] Tu coverage es >= 80%

---

## üì¶ C√≥mo se Us√≥ en el Portafolio

Los 3 proyectos del portafolio implementan testing profesional con 80%+ coverage:

### Coverage por Proyecto

| Proyecto | Coverage | Tests | Archivos Clave |
|----------|:--------:|:-----:|----------------|
| BankChurn | 79%+ | 45+ | `tests/conftest.py`, `test_pipeline.py` |
| CarVision | 97% | 50+ | `tests/test_features.py`, `test_data.py` |
| TelecomAI | 97% | 35+ | `tests/test_training.py` |

### conftest.py Real (CarVision)

```python
# CarVision-Market-Intelligence/tests/conftest.py
import pytest
import pandas as pd

@pytest.fixture
def sample_df():
    """DataFrame de prueba con datos realistas."""
    return pd.DataFrame({
        'model': ['Ford F-150', 'Toyota Camry'],
        'model_year': [2020, 2019],
        'odometer': [50000, 30000],
        'price': [35000, 25000]
    })

@pytest.fixture
def config():
    """Configuraci√≥n de prueba."""
    return {
        'data': {'target_column': 'price'},
        'model': {'random_state': 42}
    }
```

### Estructura de Tests

```
tests/
‚îú‚îÄ‚îÄ conftest.py           # Fixtures compartidas
‚îú‚îÄ‚îÄ test_config.py        # Tests de configuraci√≥n
‚îú‚îÄ‚îÄ test_data.py          # Tests de carga/validaci√≥n
‚îú‚îÄ‚îÄ test_features.py      # Tests de FeatureEngineer
‚îú‚îÄ‚îÄ test_pipeline.py      # Tests de pipeline
‚îú‚îÄ‚îÄ test_training.py      # Tests de entrenamiento
‚îî‚îÄ‚îÄ test_api.py           # Tests de FastAPI
```

### üîß Ejercicio: Ejecuta Tests Reales

```bash
# 1. Ejecuta tests de CarVision
cd CarVision-Market-Intelligence
pytest tests/ -v --cov=src/carvision --cov-report=term-missing

# 2. Ve qu√© l√≠neas NO est√°n cubiertas
pytest --cov-report=html  # Genera htmlcov/index.html

# 3. Compara con BankChurn
cd ../BankChurn-Predictor
pytest tests/ -v --cov=src/bankchurn
```

---

## üíº Consejos Profesionales

> **Recomendaciones para destacar en entrevistas y proyectos reales**

### Para Entrevistas

1. **Testing ML es diferente**: Explica tests de datos, modelo, y serving (no solo c√≥digo).

2. **Great Expectations**: Menciona que usas validaci√≥n de datos como parte del pipeline.

3. **Coverage no es todo**: Un modelo con 100% coverage puede fallar en producci√≥n.

### Para Proyectos Reales

| Tipo de Test | Qu√© Verificar |
|--------------|---------------|
| Data Tests | Schema, rangos, distribuciones, nulls |
| Model Tests | M√©tricas m√≠nimas, overfitting, invariancia |
| Integration | Pipeline end-to-end, API responses |
| Performance | Latencia, throughput, memory |

### Estrategia de Testing ML

```
Unit Tests:     Transformadores individuales
Integration:    Pipeline completo con datos sint√©ticos
Validation:     M√©tricas en holdout real
Monitoring:     Drift detection en producci√≥n
```


---

## üì∫ Recursos Externos Recomendados

> Ver [RECURSOS_POR_MODULO.md](RECURSOS_POR_MODULO.md) para la lista completa.

| üè∑Ô∏è | Recurso | Tipo |
|:--:|:--------|:-----|
| üî¥ | [pytest Tutorial - ArjanCodes](https://www.youtube.com/watch?v=cHYq1MRoyI0) | Video |
| üü° | [Testing for Data Science - Eric Ma](https://www.youtube.com/watch?v=0ysyWk-ox-8) | Video |

**Documentaci√≥n oficial:**
- [pytest Documentation](https://docs.pytest.org/)
- [pytest-cov](https://pytest-cov.readthedocs.io/)
- [Great Expectations](https://greatexpectations.io/) - Data validation

---

## üîó Referencias del Glosario

Ver [21_GLOSARIO.md](21_GLOSARIO.md) para definiciones de:
- **conftest.py**: Fixtures compartidas de pytest
- **Coverage**: Porcentaje de c√≥digo ejecutado por tests
- **Fixture**: Setup reutilizable para tests

---

## ‚úÖ Ejercicios

Ver [EJERCICIOS.md](EJERCICIOS.md) - M√≥dulo 11:
- **11.1**: Test de validaci√≥n de datos
- **11.2**: Test de pipeline ML

---

<div align="center">

[‚Üê Volver al √çndice](00_INDICE.md) | [Siguiente: CI/CD ‚Üí](12_CI_CD.md)

</div>
