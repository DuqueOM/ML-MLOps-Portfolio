# üéØ Simulacro Entrevista Lead/Senior ML Engineer - Parte 2

**Continuaci√≥n de preguntas 41-115**

---

# Preguntas 41-45: BankChurn (continuaci√≥n)

## Pregunta 41: Risk Level Classification
**¬øC√≥mo categorizas el riesgo de churn?**

### Respuesta:
```python
# prediction.py
def _assign_risk_level(self, probability: float) -> str:
    if probability < 0.3:
        return "low"
    elif probability < 0.7:
        return "medium"
    else:
        return "high"
```

**Uso en negocio**:
- **High**: Contacto proactivo inmediato
- **Medium**: Ofertas de retenci√≥n
- **Low**: Monitoreo est√°ndar

---

## Pregunta 42: Feature Contribution Explanation
**¬øC√≥mo explicas las predicciones?**

### Respuesta:
```python
# prediction.py
def _generate_explanation(self, X_row) -> Dict[str, float]:
    # Enfoque simplificado: distancia a media por feature
    contributions = {}
    for col in self.feature_names:
        mean_val = self.feature_means.get(col, 0)
        diff = (X_row[col] - mean_val) / (self.feature_stds.get(col, 1) + 1e-8)
        contributions[col] = float(diff)
    return contributions
```

**Limitaci√≥n reconocida**: No es SHAP values real, pero da intuici√≥n inicial sin dependencia adicional.

---

## Pregunta 43: Model Metadata
**¬øQu√© metadata guardas con el modelo?**

### Respuesta:
```json
{
  "version": "1.0.0",
  "trained_at": "2024-11-20T10:30:00Z",
  "config_hash": "abc123def456",
  "test_metrics": {
    "f1_score": 0.82,
    "auc_roc": 0.853
  },
  "feature_names": ["CreditScore", "Age", ...],
  "training_samples": 8000,
  "git_commit": "abc123"
}
```

**Uso**: Trazabilidad, comparaci√≥n entre versiones, debugging.

---

## Pregunta 44: Lifespan vs On-Event Loading
**¬øPor qu√© usas lifespan en lugar de startup event?**

### Respuesta:
```python
# FastAPI >= 0.95 depreca @app.on_event
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    load_model()
    yield
    # Shutdown (cleanup)
    
app = FastAPI(lifespan=lifespan)
```

**Beneficios**:
- Patr√≥n moderno recomendado
- Context manager claro para setup/teardown
- Mejor manejo de recursos async

---

## Pregunta 45: Test Coverage Strategy
**¬øC√≥mo alcanzaste 77% coverage?**

### Respuesta:
```python
# pytest.ini
[pytest]
addopts = --cov=src/bankchurn --cov-report=term-missing

# Estrategia:
# 1. Unit tests por m√≥dulo
# 2. Integration tests para workflows completos
# 3. Edge cases: datos vac√≠os, missing columns, tipos inv√°lidos
# 4. Error paths: excepciones esperadas
```

**Cobertura por m√≥dulo**:
- training.py: 85%
- evaluation.py: 80%
- config.py: 90%
- cli.py: 60% (I/O dif√≠cil de testear)

---

# Preguntas 51-60: CarVision (continuaci√≥n)

## Pregunta 51: Feature Type Inference
**¬øC√≥mo detectas tipos de features autom√°ticamente?**

### Respuesta:
```python
# data.py
def infer_feature_types(df, cfg):
    num_cfg = cfg.get("numeric_features", [])
    cat_cfg = cfg.get("categorical_features", [])
    
    if not num_cfg:
        num_cfg = df.select_dtypes(include=["int64", "float64"]).columns.tolist()
    if not cat_cfg:
        cat_cfg = df.select_dtypes(include=["object", "category"]).columns.tolist()
    
    return num_cfg, cat_cfg
```

**Beneficio**: Config minimalista (`[]`) usa inferencia autom√°tica.

---

## Pregunta 52: Segment Error Analysis
**¬øC√≥mo analizas errores por segmento?**

### Respuesta:
```python
def _analyse_errors_by_segment(df, y_true, y_pred, segment_cols):
    results = []
    for col in segment_cols:  # ["condition", "type", "model_year"]
        for val, group in df.groupby(col):
            if len(group) < 30:
                continue
            results.append({
                "segment": f"{col}={val}",
                "rmse": rmse(group[target], y_pred[group.index]),
                "n_samples": len(group)
            })
    return pd.DataFrame(results)
```

**Hallazgo t√≠pico**: Mayor error en autos muy nuevos (pocos datos) o muy viejos (alta variabilidad).

---

## Pregunta 53: Dual Application (API + Dashboard)
**¬øC√≥mo manejas API y Streamlit juntos?**

### Respuesta:
```
app/
‚îú‚îÄ‚îÄ fastapi_app.py     # REST API (port 8002)
‚îî‚îÄ‚îÄ streamlit_app.py   # Dashboard (port 8501)
```

**Ambos usan**:
```python
from src.carvision.features import FeatureEngineer
from src.carvision.data import clean_data

# Mismo pipeline, diferentes interfaces
model = joblib.load(MODEL_PATH)
```

**Docker Compose**:
```yaml
services:
  carvision-api:
    command: uvicorn app.fastapi_app:app --port 8002
  carvision-dashboard:
    command: streamlit run app/streamlit_app.py --server.port 8501
```

---

## Pregunta 54: Price Category Leakage
**¬øPor qu√© price_category causa leakage?**

### Respuesta:
```python
# features.py - ANTES (bug)
X["price_category"] = pd.cut(X["price"], bins=[0, 5000, 15000, 50000, np.inf])
```

**Problema**: `price_category` depende de `price` (el target).
- En training: tiene el valor correcto
- En inference: `price` no existe ‚Üí error o leakage si se imputa

**Soluci√≥n**: `drop_columns: ["price_per_mile", "price_category"]` en config.

---

## Pregunta 55: Split Indices Persistence
**¬øPor qu√© guardas los √≠ndices del split?**

### Respuesta:
```python
def save_split_indices(indices, path):
    with open(path, "w") as f:
        json.dump({k: v.tolist() for k, v in indices.items()}, f)
```

**Uso**:
```python
# Reproducir exactamente el mismo split
indices = load_split_indices("split_indices.json")
X_train = X.iloc[indices["train"]]
X_test = X.iloc[indices["test"]]
```

**Beneficio**: Comparar modelos con EXACTAMENTE los mismos datos de test.

---

## Pregunta 56: Dummy Baseline
**¬øPor qu√© comparas con DummyRegressor?**

### Respuesta:
```python
dummy = DummyRegressor(strategy="median")
baseline_rmse = rmse(y_test, dummy.fit(X_train, y_train).predict(X_test))
```

**Estrategias disponibles**:
- `mean`: Predice promedio
- `median`: Predice mediana (robusta a outliers)
- `constant`: Valor fijo

**Interpretaci√≥n**: Si tu modelo no supera dummy, no agrega valor.

---

## Pregunta 57: Random Forest para Regresi√≥n
**¬øPor qu√© RF y no XGBoost para CarVision?**

### Respuesta:
| Criterio | RandomForest | XGBoost |
|----------|--------------|---------|
| **Simplicidad** | Menos hiperpar√°metros | Muchos HP cr√≠ticos |
| **Robustez** | Menos sensible a HP | Requiere tuning fino |
| **Interpretabilidad** | Feature importance directa | M√°s complejo |
| **Performance** | Competitivo en tabular | Marginalmente mejor |

**Decisi√≥n**: RF es "good enough" con menor riesgo de overfitting y configuraci√≥n m√°s simple.

---

## Pregunta 58: MAPE Calculation
**¬øPor qu√© sumas epsilon en MAPE?**

### Respuesta:
```python
mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100
```

**Problema**: Si `y_true = 0`, divisi√≥n por cero.
**Soluci√≥n**: `+ 1e-8` evita divisi√≥n por cero.

**Alternativa mejor para precios**:
```python
# Symmetric MAPE
smape = np.mean(2 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))
```

---

## Pregunta 59: Streamlit Sections
**¬øC√≥mo organizas el dashboard?**

### Respuesta:
```python
# streamlit_app.py
tab1, tab2, tab3, tab4 = st.tabs([
    "üìä Overview",
    "üìà Market Analysis", 
    "üéØ Model Metrics",
    "üí∞ Price Predictor"
])

with tab1:
    display_overview_kpis()
with tab4:
    # Formulario de predicci√≥n
    brand = st.selectbox("Brand", brands)
    year = st.slider("Year", 1990, 2024)
    if st.button("Predict"):
        pred = model.predict(features)
        st.success(f"Estimated Price: ${pred:,.0f}")
```

---

## Pregunta 60: API vs Dashboard Trade-offs
**¬øCu√°ndo recomiendas API vs Dashboard?**

### Respuesta:
| Caso de Uso | Recomendaci√≥n |
|-------------|---------------|
| Integraci√≥n con otros sistemas | API |
| Usuarios no t√©cnicos | Dashboard |
| Alto volumen | API |
| Exploraci√≥n ad-hoc | Dashboard |
| Automatizaci√≥n | API |
| Demos/POC | Dashboard |

**CarVision ofrece ambos**: API para integraci√≥n con sistemas de dealers, Dashboard para analistas de mercado.

---

# Preguntas 63-70: TelecomAI (continuaci√≥n)

## Pregunta 63: Unified Pipeline Benefit
**¬øPor qu√© un solo artefacto pipeline?**

### Respuesta:
```python
# ANTES (2 archivos)
preprocessor = joblib.load("preprocessor.pkl")
model = joblib.load("model.pkl")
X_proc = preprocessor.transform(X)
pred = model.predict(X_proc)

# AHORA (1 archivo)
pipeline = joblib.load("model.joblib")
pred = pipeline.predict(X)
```

**Beneficios**:
1. Deployment m√°s simple
2. Imposible desincronizar preprocessor/model
3. Una sola versi√≥n para auditar

---

## Pregunta 64: Config YAML Structure
**¬øC√≥mo estructuras el config de TelecomAI?**

### Respuesta:
```yaml
project_name: TelecomAI-Customer-Intelligence
random_seed: 42

paths:
  data_csv: users_behavior.csv
  model_path: artifacts/model.joblib

features: [calls, minutes, messages, mb_used]
target: is_ultra

split:
  test_size: 0.2
  stratify: true

model:
  name: gradient_boosting
  params:
    n_estimators: 200
    max_depth: 2
```

**Principio**: Toda configuraci√≥n externalizada, ning√∫n valor hardcodeado.

---

## Pregunta 65: Stratify con Clasificaci√≥n Binaria
**¬øCu√°ndo es cr√≠tica la estratificaci√≥n?**

### Respuesta:
**Cr√≠tica cuando**:
- Clases desbalanceadas (< 70/30)
- Dataset peque√±o (< 5K samples)
- M√©trica sensible a distribuci√≥n (precision, recall)

```python
# Sin estratificaci√≥n en 2K samples con 30% positivos:
# Test set podr√≠a tener 20% o 40% positivos ‚Üí m√©tricas no comparables
```

---

## Pregunta 66: Simple Model Debugging
**¬øC√≥mo debuggeas un modelo simple?**

### Respuesta:
```python
# 1. Check feature distributions
print(X_train.describe())
print(X_test.describe())  # Deber√≠a ser similar

# 2. Check target distribution
print(y_train.value_counts(normalize=True))
print(y_test.value_counts(normalize=True))

# 3. Learning curve
from sklearn.model_selection import learning_curve
train_sizes, train_scores, val_scores = learning_curve(
    model, X, y, cv=5, scoring="roc_auc"
)
```

---

## Pregunta 67: Gradient Boosting Learning Rate
**¬øQu√© pasa si lr es muy alto o muy bajo?**

### Respuesta:
| lr | Efecto |
|----|--------|
| **0.5+ (alto)** | Convergencia r√°pida, riesgo de overfitting |
| **0.01-0.1 (medio)** | Balance t√≠pico |
| **< 0.01 (bajo)** | Necesita muchos estimators, m√°s robusto |

```python
# TelecomAI usa lr=0.05, n_estimators=200
# Conservador pero estable
```

**Regla pr√°ctica**: `lr * n_estimators ‚âà 10-20` para convergencia.

---

## Pregunta 68: Evaluation Metrics Save
**¬øC√≥mo persistes m√©tricas?**

### Respuesta:
```python
# evaluation.py
def evaluate_model(pipeline, X_test, y_test, cfg):
    metrics = compute_classification_metrics(y_test, y_pred, y_proba)
    
    # Save to YAML
    with open(cfg.paths["metrics_path"], "w") as f:
        yaml.safe_dump(metrics, f)
    
    return metrics
```

**Formato YAML** para legibilidad humana:
```yaml
accuracy: 0.812
precision: 0.785
recall: 0.743
f1: 0.763
roc_auc: 0.840
```

---

## Pregunta 69: FastAPI App TelecomAI
**¬øC√≥mo estructuras la API?**

### Respuesta:
```python
# app/fastapi_app.py
class UserBehavior(BaseModel):
    calls: int
    minutes: float
    messages: int
    mb_used: float

@app.post("/predict")
async def predict_plan(user: UserBehavior):
    features = pd.DataFrame([user.dict()])
    pred = pipeline.predict(features)[0]
    proba = pipeline.predict_proba(features)[0][1]
    return {
        "recommended_plan": "Ultra" if pred == 1 else "Basic",
        "confidence": float(proba)
    }
```

---

## Pregunta 70: TelecomAI Business Context
**¬øC√≥mo se usa el modelo en el negocio?**

### Respuesta:
**Contexto**: Recomendar plan de datos (Basic vs Ultra) basado en patrones de uso.

**Flujo**:
1. Cliente usa servicio por periodo de prueba
2. Sistema recolecta m√©tricas: calls, minutes, messages, mb_used
3. Modelo predice plan √≥ptimo
4. Ventas contacta con oferta personalizada

**Valor**:
- Reduce churn por plan inadecuado
- Aumenta ARPU en usuarios infraservidos
- Mejora satisfacci√≥n del cliente

---

# Preguntas 73-80: Arquitectura (continuaci√≥n)

## Pregunta 73: Error Handling Philosophy
**¬øCu√°l es tu filosof√≠a de manejo de errores?**

### Respuesta:
1. **Fail fast**: Validar inputs al inicio
2. **Errores espec√≠ficos**: `ValueError` vs `FileNotFoundError` vs gen√©rico
3. **Logs contextuales**: Incluir datos relevantes en el error
4. **Graceful degradation**: Servicio funciona con capacidad reducida

```python
# Malo
try:
    do_something()
except Exception:
    pass

# Bueno
try:
    do_something(input_data)
except ValueError as e:
    logger.error(f"Invalid input {input_data}: {e}")
    raise HTTPException(400, f"Invalid input: {e}")
except FileNotFoundError as e:
    logger.error(f"Model file missing: {e}")
    raise HTTPException(503, "Model not available")
```

---

## Pregunta 74: Dependency Management
**¬øC√≥mo manejas dependencias entre proyectos?**

### Respuesta:
```
Projects Tripe Ten/
‚îú‚îÄ‚îÄ common_utils/         # Shared code
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ logger.py
‚îÇ   ‚îî‚îÄ‚îÄ seed.py
‚îú‚îÄ‚îÄ BankChurn-Predictor/
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt  # Project-specific deps
‚îú‚îÄ‚îÄ CarVision-Market-Intelligence/
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ TelecomAI-Customer-Intelligence/
    ‚îî‚îÄ‚îÄ requirements.txt
```

**common_utils** en cada project:
```python
import sys
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from common_utils.seed import set_seed
```

---

## Pregunta 75: API Versioning
**¬øC√≥mo versionar√≠as la API?**

### Respuesta:
```python
# Opci√≥n 1: Path versioning
@app.post("/v1/predict")
@app.post("/v2/predict")

# Opci√≥n 2: Header versioning
@app.post("/predict")
async def predict(request: Request):
    version = request.headers.get("API-Version", "1")
    
# Opci√≥n 3: Query param
@app.post("/predict")
async def predict(version: str = Query("1")):
```

**Recomendaci√≥n**: Path versioning es m√°s expl√≠cito y cacheable.

---

## Pregunta 76: Configuration Hierarchy
**¬øC√≥mo manejas diferentes environments?**

### Respuesta:
```yaml
# configs/config.yaml (base)
mlflow:
  tracking_uri: "file:./mlruns"  # Default: local

# Override via environment variables
# MLFLOW_TRACKING_URI=http://mlflow.prod:5000

# O archivos separados
# configs/config.dev.yaml
# configs/config.prod.yaml
```

**Carga con override**:
```python
config = BankChurnConfig.from_yaml("configs/config.yaml")
config.mlflow.tracking_uri = os.getenv("MLFLOW_URI", config.mlflow.tracking_uri)
```

---

## Pregunta 77: Async vs Sync en FastAPI
**¬øCu√°ndo usas async?**

### Respuesta:
```python
# Sync - operaciones CPU-bound (ML inference)
@app.post("/predict")
def predict_sync(data: CustomerData):
    return model.predict(data)  # CPU bound

# Async - operaciones I/O bound
@app.get("/health")
async def health_async():
    await check_external_service()  # Network call
```

**ML inference es CPU-bound**: `def` es preferible para evitar bloquear event loop.

---

## Pregunta 78: Testing Pyramid
**¬øC√≥mo estructuras tus tests?**

### Respuesta:
```
Tests/
‚îú‚îÄ‚îÄ Unit (70%)
‚îÇ   ‚îú‚îÄ‚îÄ test_config.py
‚îÇ   ‚îú‚îÄ‚îÄ test_models.py
‚îÇ   ‚îî‚îÄ‚îÄ test_evaluation.py
‚îú‚îÄ‚îÄ Integration (20%)
‚îÇ   ‚îî‚îÄ‚îÄ test_training.py (E2E workflow)
‚îî‚îÄ‚îÄ E2E (10%)
    ‚îî‚îÄ‚îÄ test_api.py (requests reales)
```

**Pir√°mide**: Muchos unit tests (r√°pidos), pocos E2E (lentos pero valiosos).

---

## Pregunta 79: Code Review Checklist
**¬øQu√© revisas en un PR de ML?**

### Respuesta:
1. **Data leakage**: ¬øSplit antes de fit?
2. **Reproducibilidad**: ¬øSeeds configurados?
3. **Tests**: ¬øCubren happy path y edge cases?
4. **Config**: ¬øValores hardcodeados?
5. **M√©tricas**: ¬øApropiadas para el problema?
6. **Logging**: ¬øSuficiente para debugging?
7. **Documentation**: ¬øModel card actualizado?

---

## Pregunta 80: Technical Debt Management
**¬øC√≥mo manejas deuda t√©cnica?**

### Respuesta:
**Categorizaci√≥n**:
- **Critical**: Bugs de seguridad, data leakage ‚Üí Sprint actual
- **High**: Tests faltantes, logging pobre ‚Üí Pr√≥ximo sprint
- **Medium**: Refactoring, documentaci√≥n ‚Üí Backlog priorizado
- **Low**: Nice-to-have ‚Üí Tech debt day mensual

**Tracking**: Issues en GitHub con label `tech-debt` y estimaci√≥n de impacto.

---

# Preguntas 83-90: CI/CD (continuaci√≥n)

## Pregunta 83: Matrix Testing Strategy
**¬øPor qu√© matrix Python 3.11/3.12?**

### Respuesta:
```yaml
strategy:
  matrix:
    python-version: ['3.11', '3.12']
    project: [BankChurn, CarVision, TelecomAI]
```

**Razones**:
- **3.11**: Versi√≥n estable ampliamente usada
- **3.12**: Versi√≥n m√°s reciente, validar compatibilidad
- **3 proyectos**: Detectar regresiones cross-project

**Total jobs**: 2 √ó 3 = 6 combinaciones paralelas.

---

## Pregunta 84: Fail-Fast Strategy
**¬øCu√°ndo usar fail-fast: false?**

### Respuesta:
```yaml
strategy:
  fail-fast: false  # Contin√∫a aunque un job falle
```

**Usar `false` cuando**:
- Quieres ver TODOS los errores, no solo el primero
- Jobs son independientes (matriz de versiones)
- Debugging de problemas de compatibilidad

**Usar `true` cuando**:
- Jobs dependientes
- Quieres feedback r√°pido
- Recursos de CI limitados

---

## Pregunta 85: Docker Build Caching
**¬øC√≥mo optimizas builds de Docker?**

### Respuesta:
```dockerfile
# Orden √≥ptimo de COPY
COPY requirements.txt .
RUN pip install -r requirements.txt  # Cached si requirements no cambia
COPY src/ ./src/                      # Solo copia c√≥digo
```

**CI caching**:
```yaml
- uses: docker/build-push-action@v5
  with:
    cache-from: type=gha
    cache-to: type=gha,mode=max
```

---

## Pregunta 86: Security Scanning Results
**¬øQu√© haces con findings de seguridad?**

### Respuesta:
**Proceso**:
1. **Critical/High**: Bloquea PR, fix inmediato
2. **Medium**: Documenta, fix en siguiente sprint
3. **Low**: Backlog, evaluar riesgo/esfuerzo
4. **False positives**: A√±adir a `.gitleaksignore` con justificaci√≥n

```yaml
# .gitleaksignore
# False positive: example API key in documentation
docs/examples/api_usage.md:15
```

---

## Pregunta 87: Coverage Thresholds
**¬øPor qu√© 70% coverage m√≠nimo?**

### Respuesta:
```yaml
- name: Check coverage threshold
  run: |
    coverage report --fail-under=70
```

**Raz√≥n del 70%**:
- **< 60%**: Riesgo de bugs no detectados
- **70-80%**: Balance costo/beneficio t√≠pico
- **> 90%**: Diminishing returns, tests fr√°giles

**No todo necesita tests**: I/O, logging, error messages triviales.

---

## Pregunta 88: Integration Test Strategy
**¬øQu√© cubren tus integration tests?**

### Respuesta:
```yaml
integration-test:
  steps:
    - run: docker-compose -f docker-compose.demo.yml up -d
    - run: |
        # Wait for services
        sleep 30
        # Test each API
        curl http://localhost:8001/health
        curl http://localhost:8002/health
        curl -X POST http://localhost:8001/predict -d '...'
```

**Cobertura**: Health checks, predicciones b√°sicas, formato de respuesta.

---

## Pregunta 89: Documentation Validation
**¬øC√≥mo validas documentaci√≥n?**

### Respuesta:
```yaml
doc-validation:
  steps:
    - name: Check markdown links
      run: |
        npm install -g markdown-link-check
        find . -name "*.md" -exec markdown-link-check {} \;
    
    - name: Build mkdocs
      run: |
        pip install mkdocs
        mkdocs build --strict
```

**Valida**: Links rotos, sintaxis markdown, build de docs.

---

## Pregunta 90: Deployment Strategy
**¬øC√≥mo desplegar√≠as a producci√≥n?**

### Respuesta:
```yaml
deploy-prod:
  needs: [tests, security, docker-build]
  if: github.ref == 'refs/heads/main'
  steps:
    - name: Push to registry
      run: docker push ghcr.io/${{ github.repository }}:${{ github.sha }}
    
    - name: Update K8s deployment
      run: |
        kubectl set image deployment/bankchurn \
          bankchurn=ghcr.io/${{ github.repository }}:${{ github.sha }}
```

**Estrategia**: Rolling update con readiness probes.

---

# Preguntas 93-100: Infraestructura (continuaci√≥n)

## Pregunta 93: Resource Requests vs Limits
**Explica requests vs limits en K8s.**

### Respuesta:
```yaml
resources:
  requests:
    memory: "512Mi"  # Garantizado
    cpu: "250m"      # Scheduler usa esto para placement
  limits:
    memory: "1Gi"    # M√°ximo (OOMKill si excede)
    cpu: "1000m"     # Throttling si excede
```

**Best practice**:
- `requests`: Uso t√≠pico (P50)
- `limits`: Uso pico aceptable (P99)

---

## Pregunta 94: Prometheus Scraping
**¬øC√≥mo configuras Prometheus para scraping?**

### Respuesta:
```yaml
# prometheus-config.yaml
scrape_configs:
  - job_name: 'bankchurn-predictor'
    kubernetes_sd_configs:
    - role: pod
      namespaces:
        names: [ml-portfolio]
    relabel_configs:
    - source_labels: [__meta_kubernetes_pod_label_app]
      action: keep
      regex: bankchurn-predictor
```

**Annotations en Deployment**:
```yaml
annotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "8000"
  prometheus.io/path: "/metrics"
```

---

## Pregunta 95: ConfigMap Usage
**¬øCu√°ndo usas ConfigMap vs Secret?**

### Respuesta:
| Dato | Usar |
|------|------|
| MODEL_VERSION | ConfigMap |
| LOG_LEVEL | ConfigMap |
| API_KEY | Secret |
| DB_PASSWORD | Secret |

```yaml
# ConfigMap
apiVersion: v1
kind: ConfigMap
data:
  MODEL_VERSION: "v2.0.0"
  LOG_LEVEL: "INFO"
---
# Montaje en pod
envFrom:
  - configMapRef:
      name: bankchurn-config
```

---

## Pregunta 96: Rolling Update Strategy
**Explica tu estrategia de actualizaci√≥n.**

### Respuesta:
```yaml
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1  # M√°ximo 1 pod down
      maxSurge: 1        # M√°ximo 1 pod extra temporal
```

**Flujo con 3 replicas**:
1. Crear 1 nuevo pod (4 total)
2. Cuando nuevo est√° Ready, terminar 1 viejo (3 total)
3. Repetir hasta todos actualizados

**Zero downtime** con readiness probes correctos.

---

## Pregunta 97: Volume Mounts para Modelos
**¬øC√≥mo montas modelos en K8s?**

### Respuesta:
```yaml
spec:
  containers:
  - name: bankchurn-api
    volumeMounts:
    - name: models
      mountPath: /app/models
      readOnly: true
  volumes:
  - name: models
    persistentVolumeClaim:
      claimName: ml-models-pvc
```

**Alternativas**:
- **PVC**: Modelos compartidos entre pods
- **S3/GCS**: Descarga al inicio
- **ConfigMap**: Solo para configs peque√±os

---

## Pregunta 98: Ingress Configuration
**¬øC√≥mo expones servicios externamente?**

### Respuesta:
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ml-portfolio-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: bankchurn.example.com
    http:
      paths:
      - path: /
        backend:
          service:
            name: bankchurn-service
            port:
              number: 8000
```

---

## Pregunta 99: Terraform Overview
**¬øQu√© infraestructura defines con Terraform?**

### Respuesta:
```
infra/terraform/
‚îú‚îÄ‚îÄ aws/
‚îÇ   ‚îú‚îÄ‚îÄ main.tf      # EKS cluster, S3, RDS
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îî‚îÄ‚îÄ outputs.tf
‚îî‚îÄ‚îÄ gcp/
    ‚îú‚îÄ‚îÄ main.tf      # GKE, GCS, CloudSQL
    ‚îî‚îÄ‚îÄ ...
```

**Recursos t√≠picos**:
- Container registry (ECR/GCR)
- Kubernetes cluster (EKS/GKE)
- Object storage (S3/GCS) para modelos
- Database (RDS/CloudSQL) para MLflow

---

## Pregunta 100: Disaster Recovery
**¬øC√≥mo manejas DR para ML systems?**

### Respuesta:
**1. Model artifacts**:
- S3 versioning + cross-region replication
- Git LFS / DVC como backup secundario

**2. MLflow database**:
- RDS multi-AZ + daily snapshots
- Point-in-time recovery

**3. Kubernetes**:
- Declarative configs en Git (GitOps)
- Multi-region deployment para HA

**RTO/RPO objetivos**:
- RTO (Recovery Time): < 1 hora
- RPO (Recovery Point): < 1 d√≠a de experimentos

---

# Preguntas 102-105: √âtica (continuaci√≥n)

## Pregunta 102: Bias Detection
**¬øC√≥mo detectas bias en producci√≥n?**

### Respuesta:
```python
# Monitoreo continuo
for segment in ["Geography", "Gender", "Age_bucket"]:
    pred_rate = predictions.groupby(segment)["churn_pred"].mean()
    log_metric(f"pred_rate_{segment}", pred_rate.to_dict())
    
    # Alerta si disparate impact < 0.8
    di = pred_rate.min() / pred_rate.max()
    if di < 0.8:
        alert(f"Potential bias in {segment}: DI={di}")
```

---

## Pregunta 103: Model Card Maintenance
**¬øCon qu√© frecuencia actualizas Model Cards?**

### Respuesta:
**Actualizar cuando**:
- Nueva versi√≥n del modelo
- Cambio en datos de entrenamiento
- Descubrimiento de limitaci√≥n/bias
- Cambio en uso previsto

**Versionado**: Model Card versi√≥n = Model versi√≥n (`v1.0.0`).

---

## Pregunta 104: Human-in-the-Loop
**¬øC√≥mo integras supervisi√≥n humana?**

### Respuesta:
**BankChurn**:
- Modelo sugiere clientes en riesgo
- Equipo de retenci√≥n revisa lista
- Decisi√≥n final es humana (llamar o no)

**No automatizar**: Ofertas, descuentos, cierre de cuentas.

---

## Pregunta 105: GDPR Compliance
**¬øC√≥mo manejas datos personales?**

### Respuesta:
1. **Minimizaci√≥n**: Solo features necesarias (no nombre, email)
2. **Pseudonimizaci√≥n**: CustomerID sin mapeo a identidad real
3. **Right to erasure**: Pipeline para eliminar datos de un cliente
4. **Audit trail**: Logs de predicciones (sin PII) para auditor√≠a

---

# Preguntas 107-115: Liderazgo (continuaci√≥n)

## Pregunta 107: Team Communication
**¬øC√≥mo comunicas decisiones t√©cnicas al equipo?**

### Respuesta:
1. **ADRs (Architecture Decision Records)**: Documentar why, not just what
2. **Tech talks**: Sesiones de 30 min sobre decisiones importantes
3. **PR descriptions**: Contexto suficiente para reviewers
4. **Diagrams**: Mermaid/Lucidchart para arquitectura

---

## Pregunta 108: Mentoring Junior Engineers
**¬øC√≥mo mentoras a juniors en ML?**

### Respuesta:
1. **Pair programming**: En primeros PRs de ML
2. **Code review detallado**: Explicar el "por qu√©"
3. **Recursos curados**: Pointing to best practices
4. **Proyectos graduales**: Simple ‚Üí Complejo

**Errores comunes a prevenir**:
- Data leakage (el m√°s cr√≠tico)
- Overfitting sin validaci√≥n
- M√©tricas incorrectas para el problema

---

## Pregunta 109: Stakeholder Management
**¬øC√≥mo manejas expectativas de stakeholders?**

### Respuesta:
1. **Baseline comparison**: "El modelo mejora X% sobre regla actual"
2. **Confidence intervals**: "Precisi√≥n entre 75-85%"
3. **Limitations expl√≠citas**: "No funciona bien para casos X"
4. **Iterative delivery**: MVP ‚Üí Mejoras incrementales

**Evitar**: Prometer 99% accuracy, plazos imposibles, omitir limitaciones.

---

## Pregunta 110: Technical Debt Negotiation
**¬øC√≥mo negocias tiempo para tech debt?**

### Respuesta:
**Argumentos efectivos**:
1. **Riesgo cuantificado**: "Sin tests, bugs llegan a prod"
2. **Velocity impact**: "Refactor ahora ahorra X horas/semana"
3. **Costo de delay**: "Cada mes aumenta esfuerzo 20%"

**Estrategia**: 20% del sprint para tech debt (negociado upfront).

---

## Pregunta 111: Production Incident Response
**¬øC√≥mo manejas un incidente en producci√≥n?**

### Respuesta:
**Playbook**:
1. **Detect**: Alertas de Prometheus/PagerDuty
2. **Triage**: Severity assessment (P1-P4)
3. **Communicate**: Status page update
4. **Mitigate**: Rollback if needed
5. **Fix**: Root cause resolution
6. **Postmortem**: Blameless an√°lisis

**Para ML espec√≠fico**: Rollback = deploy versi√≥n anterior del modelo.

---

## Pregunta 112: Cross-functional Collaboration
**¬øC√≥mo trabajas con Data Scientists vs ML Engineers?**

### Respuesta:
| Rol | Responsabilidad |
|-----|-----------------|
| **Data Scientist** | Exploraci√≥n, feature engineering, model selection |
| **ML Engineer** | Productionization, CI/CD, monitoring |
| **Overlap** | Evaluaci√≥n, experiments |

**Handoff**: DS entrega notebook + requirements, MLE convierte a pipeline.

---

## Pregunta 113: Prioritization Framework
**¬øC√≥mo priorizas features de ML?**

### Respuesta:
**RICE Score**:
- **R**each: ¬øCu√°ntos usuarios afecta?
- **I**mpact: ¬øCu√°nto mejora m√©tricas?
- **C**onfidence: ¬øQu√© tan seguros estamos?
- **E**ffort: ¬øCu√°nto trabajo requiere?

**Score = (R √ó I √ó C) / E**

---

## Pregunta 114: Remote Team Leadership
**¬øC√≥mo lideras equipos remotos?**

### Respuesta:
1. **Async by default**: Documentaci√≥n > meetings
2. **Overlap hours**: 2-3 horas para sync
3. **Clear ownership**: Cada task tiene responsable
4. **Over-communication**: Status updates frecuentes
5. **Trust + accountability**: Medir outcomes, no horas

---

## Pregunta 115: Career Growth Path
**¬øC√≥mo defines el growth path para ML Engineers?**

### Respuesta:
```
Junior ML Engineer
  ‚Üì (1-2 a√±os)
ML Engineer
  ‚Üì (2-3 a√±os)
Senior ML Engineer
  ‚Üì (2-4 a√±os)
  ‚îú‚îÄ‚Üí Staff ML Engineer (IC track)
  ‚îî‚îÄ‚Üí ML Engineering Manager (Management track)
```

**Skills por nivel**:
- **Junior**: Implementar pipelines existentes
- **Mid**: Dise√±ar nuevos pipelines
- **Senior**: Arquitectura end-to-end, mentoring
- **Staff**: Cross-team influence, technical vision

---

# üìö Resumen y Recursos

## Skills Demostrados en Este Portafolio

| √Årea | Evidencia |
|------|-----------|
| **ML Fundamentals** | 3 proyectos: clasificaci√≥n, regresi√≥n, ensemble |
| **MLOps** | MLflow, DVC, CI/CD, monitoring |
| **Software Engineering** | Pydantic, tests, modular design |
| **DevOps** | Docker, K8s, Terraform |
| **Leadership** | Documentation, decisions, trade-offs |

## Preparaci√≥n Adicional Recomendada

1. **System Design**: Dise√±ar ML system de principio a fin
2. **Coding Interview**: LeetCode medium (estructuras de datos)
3. **Behavioral**: STAR method para experiencias pasadas
4. **Deep Dive**: Estar listo para explicar CUALQUIER l√≠nea de c√≥digo

---

**Fin del Simulacro de Entrevista**

*Generado basado en an√°lisis exhaustivo del portafolio ML-MLOps*
