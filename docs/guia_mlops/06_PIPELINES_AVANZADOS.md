# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MÃ“DULO 06: PIPELINES SKLEARN AVANZADOS
# Custom Transformers, FeatureUnion y PrevenciÃ³n de Data Leakage
# GuÃ­a MLOps v5.0: Senior Edition | DuqueOM | Noviembre 2025
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

<div align="center">

# âš™ï¸ MÃ“DULO 06: Pipelines Sklearn Avanzados

### De Notebooks Experimentales a CÃ³digo Production-Ready

*"Un pipeline bien diseÃ±ado es la diferencia entre un experimento y un producto."*

| DuraciÃ³n             | TeorÃ­a               | PrÃ¡ctica             |
| :------------------: | :------------------: | :------------------: |
| **6-7 horas**        | 25%                  | 75%                  |

</div>

---

## ğŸ¯ Lo Que LograrÃ¡s en Este MÃ³dulo

1. **Construir** pipelines sklearn que encapsulan todo el preprocesamiento
2. **Crear** Custom Transformers reutilizables y testeables
3. **Evitar** Data Leakage con validaciÃ³n correcta
4. **DiseÃ±ar** pipelines modulares con ColumnTransformer y FeatureUnion

---

## 6.1 El Problema: CÃ³digo de Entrenamiento vs Inferencia

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸ˜± EL ANTI-PATRÃ“N CLÃSICO                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                               â•‘
â•‘   ENTRENAMIENTO (notebook):                                                   â•‘
â•‘   scaler = StandardScaler()                                                   â•‘
â•‘   X_train_scaled = scaler.fit_transform(X_train)                             â•‘
â•‘   encoder = OneHotEncoder()                                                   â•‘
â•‘   X_train_encoded = encoder.fit_transform(X_cat)                             â•‘
â•‘   X_train_final = np.hstack([X_train_scaled, X_train_encoded])               â•‘
â•‘   model.fit(X_train_final, y_train)                                          â•‘
â•‘   joblib.dump(model, "model.pkl")                                            â•‘
â•‘   # Â¿Y el scaler? Â¿Y el encoder? ğŸ¤”                                          â•‘
â•‘                                                                               â•‘
â•‘   INFERENCIA (otro archivo, 3 meses despuÃ©s):                                 â•‘
â•‘   model = joblib.load("model.pkl")                                           â•‘
â•‘   # ???  Â¿CÃ³mo preproceso los nuevos datos? ğŸ˜±                                â•‘
â•‘   # Â¿QuÃ© columnas? Â¿QuÃ© orden? Â¿QuÃ© scaler?                                   â•‘
â•‘                                                                               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                    âœ… LA SOLUCIÃ“N: PIPELINE UNIFICADO                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                               â•‘
â•‘   pipeline = Pipeline([                                                       â•‘
â•‘       ('preprocessor', ColumnTransformer([...])),                            â•‘
â•‘       ('model', RandomForestClassifier())                                    â•‘
â•‘   ])                                                                          â•‘
â•‘   pipeline.fit(X_train, y_train)                                             â•‘
â•‘   joblib.dump(pipeline, "pipeline.pkl")                                      â•‘
â•‘                                                                               â•‘
â•‘   # Inferencia (simple y segura):                                             â•‘
â•‘   pipeline = joblib.load("pipeline.pkl")                                     â•‘
â•‘   predictions = pipeline.predict(X_new)  # Â¡Ya sabe cÃ³mo preprocesar!        â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## 6.2 AnatomÃ­a de un Pipeline Profesional

### Estructura BÃ¡sica

```python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier

# Definir columnas
NUMERICAL_FEATURES = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']
CATEGORICAL_FEATURES = ['Geography', 'Gender']
BINARY_FEATURES = ['HasCrCard', 'IsActiveMember']

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PREPROCESADOR: ColumnTransformer
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
preprocessor = ColumnTransformer(
    transformers=[
        # (nombre, transformer, columnas)
        ('num', Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ]), NUMERICAL_FEATURES),
        
        ('cat', Pipeline([
            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
        ]), CATEGORICAL_FEATURES),
        
        ('bin', 'passthrough', BINARY_FEATURES),  # Sin transformaciÃ³n
    ],
    remainder='drop',  # Ignorar columnas no especificadas
    verbose_feature_names_out=False,
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PIPELINE COMPLETO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        random_state=42,
        class_weight='balanced',
        n_jobs=-1,
    ))
])

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# USO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Entrenamiento
pipeline.fit(X_train, y_train)

# PredicciÃ³n (incluye preprocesamiento automÃ¡ticamente)
predictions = pipeline.predict(X_test)
probabilities = pipeline.predict_proba(X_test)[:, 1]

# Guardar TODO junto
import joblib
joblib.dump(pipeline, 'models/pipeline.pkl')
```

### VisualizaciÃ³n del Pipeline

```python
from sklearn import set_config
set_config(display='diagram')

# En Jupyter, esto muestra el pipeline grÃ¡ficamente
pipeline
```

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                           PIPELINE VISUAL                                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                               â•‘
â•‘   Pipeline                                                                    â•‘
â•‘   â”œâ”€â”€ preprocessor: ColumnTransformer                                        â•‘
â•‘   â”‚   â”œâ”€â”€ num: Pipeline                                                      â•‘
â•‘   â”‚   â”‚   â”œâ”€â”€ imputer: SimpleImputer(strategy='median')                      â•‘
â•‘   â”‚   â”‚   â””â”€â”€ scaler: StandardScaler()                                       â•‘
â•‘   â”‚   â”œâ”€â”€ cat: Pipeline                                                      â•‘
â•‘   â”‚   â”‚   â”œâ”€â”€ imputer: SimpleImputer(strategy='constant')                    â•‘
â•‘   â”‚   â”‚   â””â”€â”€ encoder: OneHotEncoder(handle_unknown='ignore')                â•‘
â•‘   â”‚   â””â”€â”€ bin: passthrough                                                   â•‘
â•‘   â””â”€â”€ classifier: RandomForestClassifier(class_weight='balanced')            â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## 6.3 Custom Transformers

### Â¿Por QuÃ© Custom Transformers?

- Encapsular lÃ³gica de feature engineering
- Reutilizar entre proyectos
- Testear unitariamente
- Mantener todo en el pipeline

### Template de Custom Transformer

```python
from sklearn.base import BaseEstimator, TransformerMixin
from typing import Optional
import pandas as pd
import numpy as np

class FeatureEngineer(BaseEstimator, TransformerMixin):
    """
    Custom transformer para feature engineering.
    
    Sigue la API de sklearn:
    - fit(X, y=None) â†’ self
    - transform(X) â†’ X_transformed
    - fit_transform(X, y=None) â†’ X_transformed
    
    Attributes:
        feature_names_out_: List[str] - Nombres de features despuÃ©s de transform
    """
    
    def __init__(
        self,
        add_ratios: bool = True,
        add_interactions: bool = False,
    ):
        """
        Args:
            add_ratios: Si aÃ±adir features de ratio (Balance/Salary, etc.)
            add_interactions: Si aÃ±adir interacciones entre features
        """
        self.add_ratios = add_ratios
        self.add_interactions = add_interactions
    
    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None) -> "FeatureEngineer":
        """
        Aprende parÃ¡metros necesarios del dataset de entrenamiento.
        
        En este caso, solo guardamos los nombres de columnas para validaciÃ³n.
        """
        # Validar que X es DataFrame
        if not isinstance(X, pd.DataFrame):
            raise TypeError("X debe ser un pandas DataFrame")
        
        # Guardar columnas esperadas
        self.feature_names_in_ = list(X.columns)
        
        # Calcular nombres de salida
        self.feature_names_out_ = self._compute_feature_names(X)
        
        return self
    
    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        Aplica las transformaciones aprendidas en fit.
        """
        # Validar que fit fue llamado
        if not hasattr(self, 'feature_names_in_'):
            raise RuntimeError("Debes llamar fit() antes de transform()")
        
        # Copiar para no modificar original
        X_transformed = X.copy()
        
        # AÃ±adir ratios
        if self.add_ratios:
            X_transformed = self._add_ratio_features(X_transformed)
        
        # AÃ±adir interacciones
        if self.add_interactions:
            X_transformed = self._add_interaction_features(X_transformed)
        
        return X_transformed
    
    def _add_ratio_features(self, X: pd.DataFrame) -> pd.DataFrame:
        """AÃ±ade features de ratio."""
        # Balance por producto
        if 'Balance' in X.columns and 'NumOfProducts' in X.columns:
            X['BalancePerProduct'] = X['Balance'] / (X['NumOfProducts'] + 1)
        
        # Balance / Salario
        if 'Balance' in X.columns and 'EstimatedSalary' in X.columns:
            X['BalanceSalaryRatio'] = X['Balance'] / (X['EstimatedSalary'] + 1)
        
        # Tenure / Age
        if 'Tenure' in X.columns and 'Age' in X.columns:
            X['TenureAgeRatio'] = X['Tenure'] / (X['Age'] + 1)
        
        return X
    
    def _add_interaction_features(self, X: pd.DataFrame) -> pd.DataFrame:
        """AÃ±ade features de interacciÃ³n."""
        if 'Age' in X.columns and 'Balance' in X.columns:
            X['Age_x_Balance'] = X['Age'] * X['Balance']
        return X
    
    def _compute_feature_names(self, X: pd.DataFrame) -> list:
        """Calcula los nombres de features de salida."""
        names = list(X.columns)
        
        if self.add_ratios:
            if 'Balance' in X.columns and 'NumOfProducts' in X.columns:
                names.append('BalancePerProduct')
            if 'Balance' in X.columns and 'EstimatedSalary' in X.columns:
                names.append('BalanceSalaryRatio')
            if 'Tenure' in X.columns and 'Age' in X.columns:
                names.append('TenureAgeRatio')
        
        if self.add_interactions:
            if 'Age' in X.columns and 'Balance' in X.columns:
                names.append('Age_x_Balance')
        
        return names
    
    def get_feature_names_out(self, input_features: Optional[list] = None) -> np.ndarray:
        """Retorna nombres de features de salida (API sklearn 1.0+)."""
        return np.array(self.feature_names_out_)
```

### Ejemplo: TargetEncoder Custom

```python
from sklearn.base import BaseEstimator, TransformerMixin
import pandas as pd
import numpy as np

class TargetEncoder(BaseEstimator, TransformerMixin):
    """
    Target Encoder que evita data leakage usando leave-one-out.
    
    Para cada categorÃ­a, calcula la media del target (con smoothing).
    """
    
    def __init__(self, smoothing: float = 10.0, min_samples: int = 5):
        self.smoothing = smoothing
        self.min_samples = min_samples
    
    def fit(self, X: pd.DataFrame, y: pd.Series) -> "TargetEncoder":
        """Calcula encodings para cada categorÃ­a."""
        self.encodings_ = {}
        self.global_mean_ = y.mean()
        
        for col in X.columns:
            # Calcular stats por categorÃ­a
            stats = pd.DataFrame({
                'category': X[col],
                'target': y
            }).groupby('category').agg({
                'target': ['mean', 'count']
            })
            stats.columns = ['mean', 'count']
            
            # Smoothing: blend con media global
            # encoding = (count * mean + smoothing * global_mean) / (count + smoothing)
            smoothed = (
                stats['count'] * stats['mean'] + 
                self.smoothing * self.global_mean_
            ) / (stats['count'] + self.smoothing)
            
            # Para categorÃ­as con pocas muestras, usar global mean
            smoothed[stats['count'] < self.min_samples] = self.global_mean_
            
            self.encodings_[col] = smoothed.to_dict()
        
        return self
    
    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        """Aplica target encoding."""
        X_encoded = X.copy()
        
        for col in X.columns:
            X_encoded[col] = X[col].map(self.encodings_.get(col, {}))
            # CategorÃ­as no vistas â†’ global mean
            X_encoded[col].fillna(self.global_mean_, inplace=True)
        
        return X_encoded
```

---

## 6.4 PrevenciÃ³n de Data Leakage

### Â¿QuÃ© es Data Leakage?

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                          âš ï¸ DATA LEAKAGE                                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                               â•‘
â•‘   DEFINICIÃ“N:                                                                 â•‘
â•‘   Cuando informaciÃ³n del futuro o del test set "se filtra" al                 â•‘
â•‘   entrenamiento, causando mÃ©tricas infladas que no se reproducen              â•‘
â•‘   en producciÃ³n.                                                              â•‘
â•‘                                                                               â•‘
â•‘   SÃNTOMA CLÃSICO:                                                            â•‘
â•‘   â€¢ AUC en desarrollo: 0.98 ğŸ‰                                                â•‘
â•‘   â€¢ AUC en producciÃ³n: 0.65 ğŸ˜±                                                â•‘
â•‘                                                                               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                               â•‘
â•‘   TIPOS DE LEAKAGE:                                                           â•‘
â•‘                                                                               â•‘
â•‘   1. Target Leakage:                                                          â•‘
â•‘      Feature que contiene informaciÃ³n del target                              â•‘
â•‘      Ej: "fecha_de_cancelaciÃ³n" para predecir churn                          â•‘
â•‘                                                                               â•‘
â•‘   2. Train-Test Contamination:                                                â•‘
â•‘      Preprocesar con datos de test                                           â•‘
â•‘      Ej: StandardScaler().fit(X_all) antes del split                         â•‘
â•‘                                                                               â•‘
â•‘   3. Temporal Leakage:                                                        â•‘
â•‘      Usar datos del futuro para predecir el pasado                           â•‘
â•‘      Ej: Random split en series temporales                                   â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Errores Comunes y Soluciones

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# âŒ ERROR 1: Escalar ANTES del split
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# MAL
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # â† Usa estadÃ­sticas de TODO el dataset
X_train, X_test = train_test_split(X_scaled, ...)

# BIEN
X_train, X_test = train_test_split(X, ...)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # Solo train
X_test_scaled = scaler.transform(X_test)  # Solo transform

# MEJOR (Pipeline lo hace automÃ¡ticamente correcto)
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', LogisticRegression())
])
pipeline.fit(X_train, y_train)  # scaler.fit solo ve X_train

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# âŒ ERROR 2: Feature selection con todo el dataset
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# MAL
from sklearn.feature_selection import SelectKBest
selector = SelectKBest(k=10)
X_selected = selector.fit_transform(X, y)  # â† Leakage
X_train, X_test = train_test_split(X_selected, ...)

# BIEN (dentro del pipeline)
pipeline = Pipeline([
    ('selector', SelectKBest(k=10)),
    ('model', RandomForestClassifier())
])
pipeline.fit(X_train, y_train)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# âŒ ERROR 3: Imputar con media de todo el dataset
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# MAL
X['Age'].fillna(X['Age'].mean(), inplace=True)  # Media de TODO
X_train, X_test = train_test_split(X, ...)

# BIEN
X_train, X_test = train_test_split(X, ...)
train_mean = X_train['Age'].mean()
X_train['Age'].fillna(train_mean, inplace=True)
X_test['Age'].fillna(train_mean, inplace=True)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# âŒ ERROR 4: Target encoding fuera del pipeline
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# MAL
encoder = TargetEncoder()
X['Geography'] = encoder.fit_transform(X[['Geography']], y)  # â† Leakage
X_train, X_test = train_test_split(X, ...)

# BIEN (Target encoding DEBE estar en el pipeline)
pipeline = Pipeline([
    ('encoder', TargetEncoder()),  # fit solo con train
    ('model', RandomForestClassifier())
])
```

### Cross-Validation Correcta

```python
from sklearn.model_selection import cross_val_score, StratifiedKFold

# El pipeline COMPLETO debe ir dentro del CV
# AsÃ­ el preprocesamiento se re-fitea en cada fold

pipeline = Pipeline([
    ('preprocessor', ColumnTransformer([...])),
    ('model', RandomForestClassifier())
])

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Esto es correcto: el pipeline se fitea 5 veces, cada vez solo con train fold
scores = cross_val_score(pipeline, X, y, cv=cv, scoring='roc_auc')
print(f"AUC: {scores.mean():.4f} (+/- {scores.std()*2:.4f})")
```

### ValidaciÃ³n Temporal (Time Series)

```python
from sklearn.model_selection import TimeSeriesSplit

# Para datos con componente temporal, NUNCA usar random split
# El futuro no puede predecir el pasado

tscv = TimeSeriesSplit(n_splits=5)

for train_idx, test_idx in tscv.split(X):
    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
    
    # Verificar que test siempre es despuÃ©s de train
    print(f"Train: {X_train.index.min()} - {X_train.index.max()}")
    print(f"Test:  {X_test.index.min()} - {X_test.index.max()}")
```

---

## 6.5 Pipeline Completo para BankChurn

```python
# src/bankchurn/pipeline.py
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.base import BaseEstimator, TransformerMixin
import pandas as pd
import numpy as np
from typing import List, Optional

class FeatureEngineer(BaseEstimator, TransformerMixin):
    """Feature engineering para BankChurn."""
    
    def __init__(self, add_ratios: bool = True):
        self.add_ratios = add_ratios
    
    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None) -> "FeatureEngineer":
        self.feature_names_in_ = list(X.columns)
        return self
    
    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X = X.copy()
        if self.add_ratios:
            X['BalancePerProduct'] = X['Balance'] / (X['NumOfProducts'].clip(lower=1))
            X['BalanceSalaryRatio'] = X['Balance'] / (X['EstimatedSalary'].clip(lower=1))
        return X
    
    def get_feature_names_out(self, input_features=None):
        names = list(self.feature_names_in_)
        if self.add_ratios:
            names.extend(['BalancePerProduct', 'BalanceSalaryRatio'])
        return np.array(names)


def build_pipeline(
    numerical_features: List[str],
    categorical_features: List[str],
    binary_features: List[str],
    model_params: Optional[dict] = None,
) -> Pipeline:
    """
    Construye el pipeline completo de BankChurn.
    
    Args:
        numerical_features: Lista de features numÃ©ricas
        categorical_features: Lista de features categÃ³ricas
        binary_features: Lista de features binarias (0/1)
        model_params: ParÃ¡metros para RandomForestClassifier
    
    Returns:
        Pipeline sklearn completo
    """
    model_params = model_params or {
        'n_estimators': 100,
        'max_depth': 10,
        'random_state': 42,
        'class_weight': 'balanced',
        'n_jobs': -1,
    }
    
    # Preprocessor para features originales
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', Pipeline([
                ('imputer', SimpleImputer(strategy='median')),
                ('scaler', StandardScaler()),
            ]), numerical_features),
            
            ('cat', Pipeline([
                ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),
                ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False)),
            ]), categorical_features),
            
            ('bin', 'passthrough', binary_features),
        ],
        remainder='drop',
    )
    
    # Pipeline completo
    pipeline = Pipeline([
        ('features', FeatureEngineer(add_ratios=True)),
        ('preprocessor', preprocessor),
        ('classifier', RandomForestClassifier(**model_params)),
    ])
    
    return pipeline


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# USO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
if __name__ == "__main__":
    from bankchurn.config import load_config
    import joblib
    
    config = load_config("configs/config.yaml")
    
    pipeline = build_pipeline(
        numerical_features=config.features.numerical,
        categorical_features=config.features.categorical,
        binary_features=config.features.binary,
        model_params=config.model.model_dump(),
    )
    
    # Entrenar
    pipeline.fit(X_train, y_train)
    
    # Guardar
    joblib.dump(pipeline, config.paths.model_output)
```

---

## 6.6 Ejercicio Integrador

### Construye Tu Pipeline

1. **Identifica** las columnas de tu dataset por tipo
2. **DiseÃ±a** el ColumnTransformer
3. **Crea** al menos 1 Custom Transformer
4. **Verifica** que no hay data leakage

### Checklist de VerificaciÃ³n

```
ESTRUCTURA:
[ ] Pipeline usa ColumnTransformer para diferentes tipos de features
[ ] Preprocesamiento encapsulado (no se hace fuera del pipeline)
[ ] Pipeline guardable con joblib

CUSTOM TRANSFORMERS:
[ ] Al menos 1 transformer custom implementado
[ ] Transformer sigue API sklearn (fit/transform)
[ ] Transformer tiene get_feature_names_out()

DATA LEAKAGE:
[ ] fit() solo se llama en train data
[ ] Cross-validation usa el pipeline completo
[ ] No hay preprocesamiento antes del split
```

---

## 6.7 AutoevaluaciÃ³n

1. Â¿Por quÃ© el pipeline debe incluir el preprocesamiento y el modelo juntos?
2. Â¿CuÃ¡l es la diferencia entre `fit_transform` y `transform`?
3. Â¿Por quÃ© `handle_unknown='ignore'` es importante en OneHotEncoder?
4. Â¿CÃ³mo verificarÃ­as que no hay data leakage en tu pipeline?

---

## ğŸ”œ Siguiente Paso

Con pipelines robustos, es hora de **trackear experimentos** profesionalmente.

**[Ir a MÃ³dulo 07: Experiment Tracking â†’](07_EXPERIMENT_TRACKING.md)**

---

<div align="center">

*MÃ³dulo 06 completado. Tu pipeline ahora es production-ready.*

*Â© 2025 DuqueOM - GuÃ­a MLOps v5.0: Senior Edition*

</div>
