# üìê Template de Proyecto ML/MLOps - Est√°ndar Tier-1

Este template define la estructura est√°ndar que TODOS los proyectos del portafolio deben seguir.

---

## üìÅ Estructura de Directorios Obligatoria

```
ProjectName/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ projectname/              # Package principal
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ models.py             # Clasificadores/Regresores custom
‚îÇ       ‚îú‚îÄ‚îÄ config.py             # Pydantic configs
‚îÇ       ‚îú‚îÄ‚îÄ training.py           # Pipeline entrenamiento
‚îÇ       ‚îú‚îÄ‚îÄ evaluation.py         # M√©tricas y evaluaci√≥n
‚îÇ       ‚îú‚îÄ‚îÄ prediction.py         # Inferencia
‚îÇ       ‚îî‚îÄ‚îÄ cli.py                # CLI moderna
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ fastapi_app.py           # API REST
‚îÇ   ‚îî‚îÄ‚îÄ example_load.py          # Demo de uso
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py              # Fixtures compartidos
‚îÇ   ‚îú‚îÄ‚îÄ test_models.py           # Tests de modelos
‚îÇ   ‚îú‚îÄ‚îÄ test_config.py           # Tests de config
‚îÇ   ‚îú‚îÄ‚îÄ test_training.py         # Tests de training
‚îÇ   ‚îú‚îÄ‚îÄ test_evaluation.py       # Tests de eval
‚îÇ   ‚îú‚îÄ‚îÄ test_prediction.py       # Tests de predicci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ test_api.py              # Tests de API
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îî‚îÄ‚îÄ config.yaml              # Configuraci√≥n principal
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                     # Datos originales
‚îÇ   ‚îî‚îÄ‚îÄ preprocess.py            # Preprocessing
‚îú‚îÄ‚îÄ monitoring/
‚îÇ   ‚îî‚îÄ‚îÄ check_drift.py           # Drift detection
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ run_mlflow.py            # MLflow experiments
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ exploration.ipynb        # EDA
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ ci.yml               # CI/CD pipeline
‚îú‚îÄ‚îÄ Dockerfile                    # Containerizaci√≥n
‚îú‚îÄ‚îÄ docker-compose.yml           # Orquestaci√≥n local
‚îú‚îÄ‚îÄ Makefile                     # Comandos automatizados
‚îú‚îÄ‚îÄ pyproject.toml               # Modern Python packaging
‚îú‚îÄ‚îÄ requirements-core.txt        # Deps runtime
‚îú‚îÄ‚îÄ requirements.txt             # Deps full con hashes
‚îú‚îÄ‚îÄ README.md                    # Documentaci√≥n principal
‚îú‚îÄ‚îÄ model_card.md                # Ficha de modelo
‚îú‚îÄ‚îÄ data_card.md                 # Ficha de datos
‚îî‚îÄ‚îÄ LICENSE                      # MIT License
```

---

## üêç M√≥dulos Python Est√°ndar

### `src/projectname/models.py`
```python
"""Custom models for [PROJECT_NAME].

Implements domain-specific classifiers/regressors.
"""

from __future__ import annotations
from sklearn.base import BaseEstimator, ClassifierMixin

class CustomModel(BaseEstimator, ClassifierMixin):
    """Custom model implementation."""
    
    def __init__(self, param1: int = 42):
        self.param1 = param1
    
    def fit(self, X, y):
        # Implementation
        return self
    
    def predict(self, X):
        # Implementation
        return predictions
```

### `src/projectname/config.py`
```python
"""Configuration management with Pydantic."""

from __future__ import annotations
from pathlib import Path
from pydantic import BaseModel, Field
import yaml

class ModelConfig(BaseModel):
    """Model configuration."""
    test_size: float = Field(0.2, ge=0.0, le=1.0)
    random_state: int = 42

class DataConfig(BaseModel):
    """Data configuration."""
    target_column: str
    features: list[str] = []

class ProjectConfig(BaseModel):
    """Complete configuration."""
    model: ModelConfig
    data: DataConfig
    
    @classmethod
    def from_yaml(cls, path: str | Path) -> ProjectConfig:
        with open(path) as f:
            data = yaml.safe_load(f)
        return cls(**data)
```

### `src/projectname/training.py`
```python
"""Training pipeline."""

from __future__ import annotations
import joblib
from pathlib import Path

class Trainer:
    """Training pipeline."""
    
    def __init__(self, config: ProjectConfig):
        self.config = config
    
    def load_data(self, path: Path):
        # Load and validate
        pass
    
    def prepare_features(self, data):
        # Feature engineering
        pass
    
    def train(self, X, y):
        # Training with CV
        return model, metrics
    
    def save_model(self, path: Path):
        joblib.dump(self.model, path)
```

### `src/projectname/evaluation.py`
```python
"""Model evaluation."""

from __future__ import annotations
from sklearn.metrics import classification_report

class Evaluator:
    """Model evaluator."""
    
    def __init__(self, model, preprocessor):
        self.model = model
        self.preprocessor = preprocessor
    
    def evaluate(self, X, y):
        # Compute metrics
        return metrics
    
    def compute_fairness(self, X, y, sensitive_features):
        # Fairness analysis
        return fairness_metrics
```

### `src/projectname/prediction.py`
```python
"""Prediction pipeline."""

from __future__ import annotations
import pandas as pd

class Predictor:
    """Batch predictor."""
    
    def predict(self, X: pd.DataFrame):
        # Transform and predict
        return predictions
    
    def predict_batch(self, input_path, output_path):
        # Batch processing
        pass
```

### `src/projectname/cli.py`
```python
"""Command-line interface."""

from __future__ import annotations
import argparse
import sys

def create_parser():
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(dest="command")
    
    # train
    train = subparsers.add_parser("train")
    train.add_argument("--config", required=True)
    train.add_argument("--input", required=True)
    
    # evaluate
    eval = subparsers.add_parser("evaluate")
    # ...
    
    # predict
    predict = subparsers.add_parser("predict")
    # ...
    
    return parser

def main(argv=None):
    parser = create_parser()
    args = parser.parse_args(argv)
    
    if args.command == "train":
        # Execute training
        pass
    # ...
    
    return 0

if __name__ == "__main__":
    sys.exit(main())
```

---

## ‚úÖ pyproject.toml Est√°ndar

```toml
[build-system]
requires = ["setuptools>=65.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "projectname"
version = "1.0.0"
description = "Brief description"
requires-python = ">=3.10"
dependencies = [
    "pandas>=1.3.0",
    "numpy>=1.21.0",
    "scikit-learn>=1.0.0",
    "pydantic>=2.0.0",
    "joblib>=1.1.0",
    "pyyaml>=6.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-cov>=3.0.0",
    "black>=23.0.0",
    "mypy>=1.0.0",
]

[project.scripts]
projectname = "src.projectname.cli:main"

[tool.black]
line-length = 120

[tool.pytest.ini_options]
testpaths = ["tests"]
addopts = "-v --cov=src --cov-report=term-missing --cov-fail-under=75"

[tool.mypy]
python_version = "3.10"
ignore_missing_imports = true
```

---

## üß™ Tests Est√°ndar

### `tests/conftest.py`
```python
"""Shared fixtures."""
import pytest
from common_utils.seed import set_seed

@pytest.fixture(autouse=True)
def deterministic_seed():
    """Set seed before each test."""
    set_seed(42)
```

### `tests/test_models.py`
```python
"""Test custom models."""
import pytest
from src.projectname.models import CustomModel

def test_model_initialization():
    model = CustomModel(param1=100)
    assert model.param1 == 100

def test_model_fit_predict():
    model = CustomModel()
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    assert len(predictions) == len(X_test)
```

---

## üîÑ CI/CD Est√°ndar

### `.github/workflows/ci.yml`
```yaml
name: CI

on: [push, pull_request]

jobs:
  quality:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      - name: Install
        run: pip install -e ".[dev]"
      - name: Black
        run: black --check .
      - name: Mypy
        run: mypy src/
      - name: Tests
        run: pytest --cov=src --cov-fail-under=75
```

---

## üìù README Est√°ndar

```markdown
# Project Name

Brief description (1 sentence).

## Quick Start

\`\`\`bash
# Install
pip install -e .

# Train
projectname train --config configs/config.yaml --input data/raw/data.csv

# Predict
projectname predict --input new_data.csv --output predictions.csv
\`\`\`

## Features

- ‚úÖ Feature 1
- ‚úÖ Feature 2

## Architecture

[Diagram or description]

## Performance

| Metric | Value |
|--------|-------|
| Accuracy | 0.XX |
| F1 Score | 0.XX |

## License

MIT
```

---

## üéØ Checklist de Conformidad

### Estructura
- [ ] Tiene `src/projectname/` con 6 m√≥dulos
- [ ] Tiene `tests/` con cobertura ‚â•75%
- [ ] Tiene `pyproject.toml` moderno
- [ ] Tiene `Dockerfile` funcional

### C√≥digo
- [ ] Type hints en todas las funciones
- [ ] Docstrings estilo NumPy/Google
- [ ] Pasa black, isort, flake8, mypy
- [ ] Complejidad ciclom√°tica <10

### Tests
- [ ] Tests unitarios para cada m√≥dulo
- [ ] Tests de integraci√≥n E2E
- [ ] Tests de API
- [ ] Cobertura ‚â•75%

### CI/CD
- [ ] GitHub Actions configurado
- [ ] Tests ejecutan en m√∫ltiples OS
- [ ] Security scan (bandit)
- [ ] Docker build autom√°tico

### Documentaci√≥n
- [ ] README comprehensivo
- [ ] model_card.md
- [ ] data_card.md
- [ ] API examples

---

## üöÄ Comandos Make Est√°ndar

```makefile
.PHONY: install test lint format train api clean

install:
	pip install -e ".[dev]"

test:
	pytest --cov=src --cov-report=html

lint:
	black --check .
	mypy src/
	flake8 .

format:
	black .
	isort .

train:
	python -m src.projectname.cli train --config configs/config.yaml

api:
	uvicorn app.fastapi_app:app --reload

clean:
	find . -type d -name __pycache__ -exec rm -rf {} +
	find . -type f -name "*.pyc" -delete
```

---

Este template asegura **consistencia, calidad y profesionalismo** en todos los proyectos del portafolio.
